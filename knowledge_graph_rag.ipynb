{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "\n",
    "# load_dotenv(override=True)\n",
    "# NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "# NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "# NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "# NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\")\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# OPENAI_ENDPOINT = os.getenv(\"OPENAI_ENDPOINT\")\n",
    "# OPENAI_EMBEDDING_ENDPOINT = os.getenv(\"OPENAI_EMBEDDING_ENDPOINT\")\n",
    "\n",
    "# graph = Neo4jGraph(\n",
    "#     url=NEO4J_URI,\n",
    "#     username=NEO4J_USERNAME,\n",
    "#     password=NEO4J_PASSWORD,\n",
    "#     database=NEO4J_DATABASE,\n",
    "# )\n",
    "\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "#   MATCH (n)\n",
    "#   MATCH (m:Movie)\n",
    "#   RETURN count(n) AS numberOfNodes, count(m) AS numberOfMovies\n",
    "#   \"\"\"\n",
    "# )\n",
    "\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "# MATCH (tomCruise:Person {name: \"Tom Cruise\"})-[:ACTED_IN|DIRECTED]->(movie:Movie)\n",
    "# RETURN movie.title, movie.tagline, movie.released\n",
    "# ORDER BY movie.released DESC\n",
    "#   \"\"\"\n",
    "# )\n",
    "\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "#   CREATE VECTOR INDEX movie_tagline_embeddings IF NOT EXISTS\n",
    "#   FOR (m:Movie) ON (m.taglineEmbedding) \n",
    "#   OPTIONS { indexConfig: {\n",
    "#     `vector.dimensions`: 1536,\n",
    "#     `vector.similarity_function`: 'cosine'\n",
    "#   }}\"\"\"\n",
    "# )\n",
    "\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "#   SHOW VECTOR INDEXES\n",
    "#   \"\"\"\n",
    "# )\n",
    "\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "#     MATCH (m:Movie) \n",
    "#     WHERE m.tagline IS NOT NULL\n",
    "#     RETURN m.tagline, m.taglineEmbedding\n",
    "#     LIMIT 1        \n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# # from openai import OpenAI\n",
    "# # from tqdm import tqdm\n",
    "\n",
    "# # # Initialize OpenAI client\n",
    "# # client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# # # Fetch all movie taglines\n",
    "# # result = graph.query(\n",
    "# #     \"\"\"\n",
    "# #     MATCH (m:Movie)\n",
    "# #     WHERE m.tagline IS NOT NULL\n",
    "# #     RETURN m.title, m.tagline\n",
    "# #     \"\"\"\n",
    "# # )\n",
    "\n",
    "# # # Generate embeddings and update the graph\n",
    "# # for movie in tqdm(result):\n",
    "# #     title = movie[\"m.title\"]\n",
    "# #     tagline = movie[\"m.tagline\"]\n",
    "\n",
    "# #     # Generate embedding\n",
    "# #     response = client.embeddings.create(input=tagline, model=\"text-embedding-ada-002\")\n",
    "# #     embedding = response.data[0].embedding\n",
    "\n",
    "# #     # Update the graph\n",
    "# #     graph.query(\n",
    "# #         \"\"\"\n",
    "# #         MATCH (m:Movie {title: $title})\n",
    "# #         SET m.taglineEmbedding = $embedding\n",
    "# #         \"\"\",\n",
    "# #         params={\"title\": title, \"embedding\": embedding},\n",
    "# #     )\n",
    "\n",
    "# # print(\"Embeddings updated successfully.\")\n",
    "\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "#     MATCH (movie:Movie) WHERE movie.tagline IS NOT NULL\n",
    "#     WITH movie, genai.vector.encode(\n",
    "#         movie.tagline, \n",
    "#         \"OpenAI\", \n",
    "#         {\n",
    "#           token: $openAiApiKey,\n",
    "#           endpoint: $openAiEndpoint\n",
    "#         }) AS vector\n",
    "#     CALL db.create.setNodeVectorProperty(movie, \"taglineEmbedding\", vector)\n",
    "#     \"\"\",\n",
    "#     params={\"openAiApiKey\": OPENAI_API_KEY, \"openAiEndpoint\": OPENAI_ENDPOINT},\n",
    "# )\n",
    "\n",
    "# question = \"What movies are about love?\"\n",
    "\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "#     WITH genai.vector.encode(\n",
    "#         $question, \n",
    "#         \"OpenAI\", \n",
    "#         {\n",
    "#           token: $openAiApiKey,\n",
    "#           endpoint: $openAiEndpoint\n",
    "#         }) AS question_embedding\n",
    "#     CALL db.index.vector.queryNodes(\n",
    "#         'movie_tagline_embeddings', \n",
    "#         $top_k, \n",
    "#         question_embedding\n",
    "#         ) YIELD node AS movie, score\n",
    "#     RETURN movie.title, movie.tagline, score\n",
    "#     \"\"\",\n",
    "#     params={\n",
    "#         \"openAiApiKey\": OPENAI_API_KEY,\n",
    "#         \"openAiEndpoint\": OPENAI_ENDPOINT,\n",
    "#         \"question\": question,\n",
    "#         \"top_k\": 5,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_ENDPOINT = os.getenv(\"OPENAI_ENDPOINT\")\n",
    "OPENAI_EMBEDDING_ENDPOINT = os.getenv(\"OPENAI_EMBEDDING_ENDPOINT\")\n",
    "\n",
    "VECTOR_INDEX_NAME = \"form_10k_chunks\"\n",
    "VECTOR_NODE_LABEL = \"Chunk\"\n",
    "VECTOR_SOURCE_PROPERTY = \"text\"\n",
    "VECTOR_EMBEDDING_PROPERTY = \"textEmbedding\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=NEO4J_DATABASE,\n",
    ")\n",
    "\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "for answer in graph.query(\"SHOW CONSTRAINTS;\"):\n",
    "    graph.query(f\"DROP CONSTRAINT `{answer['name']}`;\")\n",
    "\n",
    "for answer in graph.query(\"SHOW INDEXES;\"):\n",
    "    graph.query(f\"DROP INDEX `{answer['name']}`;\")\n",
    "\n",
    "print(\"Database purged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/GM_10k/gm-20231231.html\", \"r\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "text_lst = soup.find_all(\"div\", style=\"text-align:justify;text-indent:9pt\")\n",
    "text_lst = [text.get_text().encode(\"ascii\", \"ignore\").decode(\"ascii\") for text in text_lst]\n",
    "text_lst = [text for text in text_lst if text]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    ")\n",
    "\n",
    "documents = [Document(page_content=text) for text in text_lst]\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunkId, chunk in enumerate(chunks):\n",
    "    chunkText = chunk.page_content\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MERGE(mergedChunk:Chunk {chunkId: $chunkId})\n",
    "        ON CREATE SET \n",
    "            mergedChunk.text = $chunkText,\n",
    "            mergedChunk.source = $source\n",
    "        RETURN mergedChunk  \n",
    "        \"\"\",\n",
    "        params={\"chunkId\": chunkId,\n",
    "                \"chunkText\": chunkText,\n",
    "                \"source\": \"GM_10k\"},\n",
    "    )\n",
    "\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "    CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "        FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "result = graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (n)\n",
    "    RETURN count(n) as nodeCount\n",
    "    \"\"\"\n",
    ")\n",
    "print(result)\n",
    "\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "    CREATE VECTOR INDEX `form_10k_chunks` IF NOT EXISTS\n",
    "        FOR (c:Chunk) ON (c.textEmbedding) \n",
    "        OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 1536,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "        }}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "result = graph.query(\n",
    "    \"\"\"\n",
    "    SHOW INDEXES\n",
    "    \"\"\"\n",
    ")\n",
    "print(result)\n",
    "\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (chunk:Chunk) WHERE chunk.textEmbedding IS NULL\n",
    "    WITH chunk, genai.vector.encode(\n",
    "      chunk.text, \n",
    "      \"OpenAI\", \n",
    "      {\n",
    "        token: $openAiApiKey, \n",
    "        endpoint: $openAiEndpoint\n",
    "      }) AS vector\n",
    "    CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", vector)\n",
    "    \"\"\", \n",
    "    params={\"openAiApiKey\":OPENAI_API_KEY,\n",
    "            \"openAiEndpoint\": OPENAI_ENDPOINT}\n",
    ")\n",
    "\n",
    "graph.refresh_schema()\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(question):\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  vector_search_query = \"\"\"\n",
    "    WITH genai.vector.encode(\n",
    "      $question, \n",
    "      \"OpenAI\", \n",
    "      {\n",
    "        token: $openAiApiKey,\n",
    "        endpoint: $openAiEndpoint\n",
    "      }) AS question_embedding\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, question_embedding) yield node, score\n",
    "    RETURN score, node.text AS text\n",
    "  \"\"\"\n",
    "  similar = graph.query(vector_search_query, \n",
    "                     params={\n",
    "                      'question': question, \n",
    "                      'openAiApiKey':OPENAI_API_KEY,\n",
    "                      'openAiEndpoint': OPENAI_ENDPOINT,\n",
    "                      'index_name':VECTOR_INDEX_NAME, \n",
    "                      'top_k': 10}\n",
    "                      )\n",
    "  return similar\n",
    "\n",
    "question = \"What is the company's mission?\"\n",
    "similar = neo4j_vector_search(question)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    node_label=VECTOR_NODE_LABEL,\n",
    "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
    "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
    ")\n",
    "\n",
    "retriever = neo4j_vector_store.as_retriever()\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    ChatOpenAI(temperature=0), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "question = \"How many employees does General Motors have?\"\n",
    "result = chain({\"question\": question})\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "MERGE (form:Form {formId: 0})\n",
    "    ON CREATE\n",
    "        SET form.formId = 0\n",
    "        SET form.source = $formParam.source\n",
    "RETURN form\n",
    "\"\"\",\n",
    "params={\"formParam\": {\"source\": VECTOR_SOURCE_PROPERTY}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "MATCH (chunk:Chunk)\n",
    "SET chunk.formId = 0\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "MATCH (chunk:Chunk)\n",
    "WHERE chunk.formId = $chunkFormId\n",
    "ORDER BY chunk.chunkId ASC\n",
    "WITH collect(chunk) as chunkList\n",
    "    CALL apoc.nodes.link(\n",
    "        chunkList, \n",
    "        \"NEXT\", \n",
    "        {avoidDuplicates: true}\n",
    "    )\n",
    "RETURN size(chunkList)\n",
    "\"\"\",\n",
    "params={\"chunkFormId\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "  MATCH (chunk:Chunk), (form:Form)\n",
    "    WHERE chunk.formId = form.formId\n",
    "  MERGE (chunk)-[newRelationship:PART_OF]->(form)\n",
    "  RETURN count(newRelationship)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (c1:Chunk)-[:NEXT]->(c2:Chunk)-[:NEXT]->(c3:Chunk)\n",
    "    ORDER BY c1.chunkId ASC\n",
    "    LIMIT 10\n",
    "    RETURN c1.chunkId, c2.chunkId, c3.chunkId\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH window = (c1:Chunk)-[:NEXT*0..1]->(c2:Chunk)-[:NEXT*0..1]->(c3:Chunk)\n",
    "    ORDER BY c1.chunkId ASC\n",
    "    LIMIT 10\n",
    "    RETURN c1.chunkId, c2.chunkId, c3.chunkId, length(window)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH window = (c1:Chunk)-[:NEXT*0..1]->(c2:Chunk)-[:NEXT*0..1]->(c3:Chunk)\n",
    "    ORDER BY length(window) DESC\n",
    "    LIMIT 1\n",
    "    RETURN c1.chunkId, c2.chunkId, c3.chunkId, length(window)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_query_extra_text = \"\"\"\n",
    "WITH node, score, \"BMW builds better cars than GM. \" as extraText\n",
    "RETURN extraText + \"\\n\" + node.text as text, score, node {.source} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "vector_store_extra_text = Neo4jVector.from_existing_index(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=NEO4J_DATABASE,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    text_node_property=VECTOR_SOURCE_PROPERTY,\n",
    "    retrieval_query=retrieval_query_extra_text,\n",
    ")\n",
    "\n",
    "retriever_extra_text = vector_store_extra_text.as_retriever()\n",
    "\n",
    "chain_extra_text = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    ChatOpenAI(temperature=0), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever_extra_text\n",
    ")\n",
    "\n",
    "result = chain_extra_text({\"question\": \"What does GM do and how do their products compare to BMW?\"})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.query(\n",
    "    \"\"\"\n",
    "    MATCH window=(:Chunk)-[:NEXT*0..1]->(node)-[:NEXT*0..1]->(:Chunk)\n",
    "    WITH window as longestWindow \n",
    "    ORDER BY length(window) DESC LIMIT 1\n",
    "    WITH nodes(longestWindow) as chunkList\n",
    "    UNWIND chunkList as chunkRows\n",
    "    WITH collect(chunkRows.text) as textList\n",
    "    RETURN apoc.text.join(textList, \" \\n \") as text\n",
    "\"\"\"\n",
    ")\n",
    "print(result[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_query_extra_text = \"\"\"\n",
    "  MATCH window=\n",
    "      (:Chunk)-[:NEXT*0..1]->(node)-[:NEXT*0..1]->(:Chunk)\n",
    "  WITH node, score, window as longestWindow \n",
    "    ORDER BY length(window) DESC LIMIT 1\n",
    "  WITH nodes(longestWindow) as chunkList, node, score\n",
    "    UNWIND chunkList as chunkRows\n",
    "  WITH collect(chunkRows.text) as textList, node, score\n",
    "  RETURN apoc.text.join(textList, \" \\n \") as text,\n",
    "      score,\n",
    "      node {.source} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "vector_store_extra_text = Neo4jVector.from_existing_index(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=NEO4J_DATABASE,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    text_node_property=VECTOR_SOURCE_PROPERTY,\n",
    "    retrieval_query=retrieval_query_extra_text,\n",
    ")\n",
    "\n",
    "retriever_extra_text = vector_store_extra_text.as_retriever()\n",
    "\n",
    "chain_extra_text = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    ChatOpenAI(temperature=0), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever_extra_text\n",
    ")\n",
    "\n",
    "result = chain_extra_text({\"question\": \"What does GM do and how do their products compare to BMW?\"})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (:Chunk)-[next:NEXT]->(:Chunk)\n",
    "    RETURN COUNT(next) as nextCount\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (:Chunk)-[relation]->(:Form)\n",
    "    RETURN COUNT(relation) as relationCount\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH ()-[relation]->()\n",
    "    RETURN COUNT(relation) as relationCount\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "language_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
