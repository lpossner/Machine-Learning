{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.stats import fisher_exact, chi2_contingency, skew, kurtosis\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, \\\n",
    "    OrdinalEncoder, PowerTransformer, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_classif, \\\n",
    "    mutual_info_regression\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/DontGetKicked/training.csv')\n",
    "df_train = pd.read_csv('./data/DontGetKicked/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify columns into categorical and numerical\n",
    "categorical_columns = [\n",
    "    'Auction', 'Make', 'Model', 'Trim', 'SubModel', 'Color', \n",
    "    'VehYear', 'Transmission', 'WheelType', 'Nationality', 'Size', \n",
    "    'TopThreeAmericanName', 'PRIMEUNIT', 'AUCGUART', 'VNST',\n",
    "    'WheelTypeID', 'VNZIP1', 'IsOnlineSale', 'PurchDate'\n",
    "]\n",
    "df[categorical_columns] = df[categorical_columns].astype('category')\n",
    "df_train[categorical_columns] = df_train[categorical_columns].astype('category')\n",
    "numerical_columns = list(set(df.columns) - set(categorical_columns))\n",
    "target_column =  'IsBadBuy'\n",
    "numerical_columns.remove(target_column)\n",
    "df[numerical_columns] = df[numerical_columns].astype('float')\n",
    "df_train[numerical_columns] = df_train[numerical_columns].astype('float')\n",
    "\n",
    "\n",
    "# Define helper functions\n",
    "def drop_columns(df, columns):\n",
    "    df = df.drop(columns=columns)\n",
    "    for column in columns:\n",
    "        if column in categorical_columns:\n",
    "            categorical_columns.remove(column)\n",
    "        if column in numerical_columns:\n",
    "            numerical_columns.remove(column)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_categorical_column(df, column, values):\n",
    "    df[column] = values\n",
    "    df[column] = df[column].astype('category')\n",
    "    categorical_columns.append(column)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_numerical_column(df, column, values):\n",
    "    df[column] = values\n",
    "    df[column] = df[column].astype('float')\n",
    "    numerical_columns.append(column)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_distributions(df, columns):\n",
    "    num_columns = len(columns)\n",
    "    columns_per_row = 2\n",
    "    rows = (num_columns + columns_per_row - 1) // columns_per_row\n",
    "    fig, axes = plt.subplots(rows, columns_per_row, figsize=(12, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    for index, column in enumerate(df[columns].columns):\n",
    "        sns.histplot(df[column], kde=True, ax=axes[index])\n",
    "        axes[index].set_title(f'Distribution of {column}')\n",
    "    for del_index in range(index + 1, len(axes)):\n",
    "        fig.delaxes(axes[del_index])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def nmad(df):\n",
    "    df_clean = df.dropna()\n",
    "    median = np.median(df_clean)\n",
    "    return np.median(np.abs(df_clean - median)) / median\n",
    "\n",
    "\n",
    "def cov(df):\n",
    "    df_clean = df.dropna()\n",
    "    return np.std(df_clean) / np.mean(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "unnecessary_columns = ['RefId', 'WheelTypeID', 'BYRNO',\n",
    "             'VNZIP1', 'SubModel', 'Trim', 'Model',\n",
    "             'VehYear'\n",
    "             ]    \n",
    "df = drop_columns(df=df, columns=unnecessary_columns)\n",
    "unnecessary_columns.remove('RefId')\n",
    "df_train = drop_columns(df=df_train, columns=unnecessary_columns)\n",
    "unnecessary_columns.append('RefId')\n",
    "\n",
    "# Extract month from purchase date string\n",
    "purchase_month = df['PurchDate'].apply(lambda x: x.split('/')[0])\n",
    "df = add_categorical_column(df=df, column='PurchMonth', values=purchase_month)\n",
    "df = drop_columns(df=df, columns=['PurchDate'])\n",
    "\n",
    "purchase_month = df_train['PurchDate'].apply(lambda x: x.split('/')[0])\n",
    "df_train = add_categorical_column(df=df_train, column='PurchMonth', values=purchase_month)\n",
    "df_train = drop_columns(df=df_train, columns=['PurchDate'])\n",
    "\n",
    "# Convert wheel type to upper case\n",
    "df['WheelType'] = df['WheelType'].apply(lambda x: x.upper())\n",
    "df_train['WheelType'] = df_train['WheelType'].apply(lambda x: x.upper())\n",
    "\n",
    "# Show remaining columns\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sensible value ranges and mark out-of-range-values as missing\n",
    "numerical_column_ranges = {\n",
    "    'VehicleAge': (0, 30),\n",
    "    'VehOdo': (0, 120000),\n",
    "    'VehBCost': (1000, 46000),\n",
    "    'WarrantyCost': (400, 8000),\n",
    "    'MMRAcquisitionAuctionAveragePrice': (800, 46000),\n",
    "    'MMRAcquisitionAuctionCleanPrice': (1000, 46000),\n",
    "    'MMRAcquisitionRetailAveragePrice': (1000, 46000),\n",
    "    'MMRAcquisitonRetailCleanPrice': (1000, 46000),\n",
    "    'MMRCurrentAuctionAveragePrice': (300, 46000),\n",
    "    'MMRCurrentAuctionCleanPrice': (400, 46000),\n",
    "    'MMRCurrentRetailAveragePrice': (800, 46000),\n",
    "    'MMRCurrentRetailCleanPrice': (1000, 46000)\n",
    "}\n",
    "for column, (min_value, max_value) in numerical_column_ranges.items():\n",
    "    df[column] = df[column].apply(lambda x: x if min_value <= x <= max_value else None)\n",
    "for column, (min_value, max_value) in numerical_column_ranges.items():\n",
    "    df_train[column] = df_train[column].apply(lambda x: x if min_value <= x <= max_value else None)\n",
    "\n",
    "# Drop columns with little variation\n",
    "mask = df[numerical_columns].apply(nmad) < 0.1\n",
    "columns = mask[mask].index.to_list()\n",
    "df = drop_columns(df=df, columns=columns)\n",
    "df_train = drop_columns(df=df_train, columns=columns)\n",
    "\n",
    "# Show column stats\n",
    "for column in numerical_columns:\n",
    "    print(df[column].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-sensible values\n",
    "df['Make'] = df['Make'].astype('object').replace({'TOYOTA SCION': 'SCION'}).astype('category')\n",
    "df['Transmission'] = df['Transmission'].astype('object').replace({'Manual': 'MANUAL'}).astype('category')\n",
    "df['Color'] = df['Color'].astype('object').replace({'NOT AVAIL': None}).astype('category')\n",
    "\n",
    "df_train['Make'] = df_train['Make'].astype('object').replace({'TOYOTA SCION': 'SCION'}).astype('category')\n",
    "df_train['Transmission'] = df_train['Transmission'].astype('object').replace({'Manual': 'MANUAL'}).astype('category')\n",
    "df_train['Color'] = df_train['Color'].astype('object').replace({'NOT AVAIL': None}).astype('category')\n",
    "\n",
    "# Replace rare values\n",
    "for column in categorical_columns:\n",
    "    # Categorize as 'OTHERS'\n",
    "    value_counts = df[column].value_counts()\n",
    "    proba = value_counts / value_counts.sum()\n",
    "    mask = proba <= 0.01\n",
    "    replace_dict = {key: 'OTHER' for key in mask[mask].reset_index().iloc[:, 0].to_list()}\n",
    "    df[column] = df[column].astype('object').replace(replace_dict).astype('category')\n",
    "    df_train[column] = df_train[column].astype('object').replace(replace_dict).astype('category')\n",
    "    # If frequency of 'OTHERS' is too low, mark as missing\n",
    "    num_other = len(df.loc[df[column] == 'OTHER', column])\n",
    "    if num_other / len(df[column]) < 0.01:\n",
    "        df[column] = df[column].astype('object').replace({'OTHER': None}).astype('category')\n",
    "        df_train[column] = df_train[column].astype('object').replace({'OTHER': None}).astype('category')\n",
    "\n",
    "# Drop columns with little variation\n",
    "for column in categorical_columns:\n",
    "    value_counts = df[column].value_counts()\n",
    "    proba = value_counts / value_counts.sum()\n",
    "    if (proba > 0.99).any():\n",
    "        df = drop_columns(df=df, columns=[column])\n",
    "        df_train = drop_columns(df=df_train, columns=[column])\n",
    "\n",
    "# Drop columns with too many missing values, if not correlated with target\n",
    "prob_na = df[categorical_columns].isna().sum() / len(df)\n",
    "mask = prob_na > 0.2\n",
    "na_columns = mask[mask].reset_index().iloc[:, 0].to_list()\n",
    "for column in na_columns:\n",
    "    # p_value = chi2_contingency(pd.crosstab(df[column], df[target_column])).pvalue\n",
    "    p_value = fisher_exact(pd.crosstab(df[column], df[target_column])).pvalue\n",
    "    if p_value > 0.01:\n",
    "        df = drop_columns(df=df, columns=[column])\n",
    "        df_train = drop_columns(df=df_train, columns=[column])\n",
    "\n",
    "# Show column stats\n",
    "for column in categorical_columns:\n",
    "    print(df[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for outliers\n",
    "# # Copy data frame\n",
    "# df_iso = df.drop(columns=[target_column]).copy()\n",
    "\n",
    "# # Replace missing values\n",
    "# for column in categorical_columns:\n",
    "#     df_iso[column] = df_iso[column].fillna(df_iso[column].mode().iloc[0])\n",
    "# for column in numerical_columns:\n",
    "#     df_iso[column] = df_iso[column].fillna(df_iso[column].median())\n",
    "\n",
    "# # Encode categorical values\n",
    "# one_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "# one_hot_encoded = one_hot_encoder.fit_transform(df_iso[categorical_columns])\n",
    "# df_iso = pd.concat([pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out()), df_iso[numerical_columns].reset_index(drop=True)], axis=1) \n",
    "\n",
    "# # Scale values\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(df_iso)\n",
    "\n",
    "# # Check for outliers using isolation forest\n",
    "# clf = IsolationForest()\n",
    "# outliers = clf.fit_predict(X)\n",
    "\n",
    "# # Show ratio of outliers\n",
    "# print((outliers == -1).sum() / outliers.shape[0] * 100)\n",
    "\n",
    "# # Drop outliers\n",
    "# df = df.drop(df[outliers == -1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show ratio of missing values\n",
    "# print(df.isna().any(axis=1).sum() / len(df) * 100)\n",
    "\n",
    "# # Impute missing values\n",
    "# categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "# df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
    "# numerical_imputer = SimpleImputer(strategy='median')\n",
    "# df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "# # Convert to categorical and float\n",
    "# df[categorical_columns] = df[categorical_columns].astype('category')\n",
    "# df[numerical_columns] = df[numerical_columns].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute mutual information for all combinations of numerical columns\n",
    "# column_combinations = list(combinations(numerical_columns, 2))\n",
    "# mutual_info_scores = [mutual_info_regression(df[[column_1]].values, df[[column_2]].values.squeeze()) for column_1, column_2 in column_combinations]\n",
    "# # Sort list descending\n",
    "# mutual_info_columns = sorted([(list(columns), float(score[0])) for columns, score in zip(column_combinations, mutual_info_scores)], key=lambda x: x[1], reverse=True)\n",
    "# # Iterate over 20% of the most correlated columns and choose column with the lowest variance to drop\n",
    "# column_ratio = int(len(mutual_info_columns) * 0.2)\n",
    "# columns_drop = []\n",
    "# for columns, value in mutual_info_columns[:column_ratio]:\n",
    "#     index = df[columns].var().argmin()\n",
    "#     columns_drop.append(columns[index])\n",
    "# columns_drop = list(set(columns_drop))\n",
    "# df = drop_columns(df=df, columns=columns_drop)\n",
    "\n",
    "# # Show remaining columns\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns that have low correlation with the target\n",
    "# # Numerical columns\n",
    "# selector = SelectPercentile(score_func=mutual_info_classif, percentile=30)\n",
    "# mask = selector.fit(df[numerical_columns], df[target_column]).get_support()\n",
    "# unselected_columns = [column for column, drop in zip(numerical_columns, mask) if not drop]\n",
    "# df = drop_columns(df=df, columns=unselected_columns)\n",
    "\n",
    "# # Categorical columns\n",
    "# encoder = OrdinalEncoder()\n",
    "# selector = SelectPercentile(score_func=lambda X, y: mutual_info_classif(X, y, discrete_features=True), percentile=30)\n",
    "# mask = selector.fit(encoder.fit_transform(df[categorical_columns]), df[target_column]).get_support()\n",
    "# unselected_columns = [column for column, drop in zip(categorical_columns, mask) if not drop]\n",
    "# df = drop_columns(df=df, columns=unselected_columns)\n",
    "\n",
    "# # Show remaining columns\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform non-normal distribution\n",
    "# # Plot distributions before transformation\n",
    "# plot_distributions(df, numerical_columns)\n",
    "\n",
    "# # Print Skrew and Kurtosis\n",
    "# for column in df[numerical_columns]:\n",
    "#     print(f'Column: {column}, Skew: {skew(df[column])}, Kurtosis: {kurtosis(df[column])}')\n",
    "\n",
    "# # transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal')\n",
    "# df[numerical_columns] = transformer.fit_transform(df[numerical_columns])\n",
    "\n",
    "# # Plot distributions after transformation\n",
    "# plot_distributions(df, numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split dataset in train and test data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[target_column]), df[target_column], test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = df.drop(columns=[target_column]), df.drop(columns=[target_column]), df[target_column], df[target_column]\n",
    "\n",
    "# Train classifier and predict data \n",
    "clf = XGBClassifier(enable_categorical=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy:{accuracy}')\n",
    "print(f'Presicion: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(f'{confusion_mat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(df_train.drop(columns=[\"RefId\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[\"RefId\"].astype(int)\n",
    "df_train = pd.concat([df_train, pd.DataFrame(y_pred, columns=[target_column])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../results/DontGetKicked/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
