{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(predicted, true, k):\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    hits = sum(idx in true for idx in predicted[:k])\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(predicted, true, k):\n",
    "    if not true:\n",
    "        return 0.0\n",
    "    hits = sum(idx in true for idx in predicted[:k])\n",
    "    return hits / len(true)\n",
    "\n",
    "\n",
    "def average_precision(predicted, true):\n",
    "    if not true:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    score = 0.0\n",
    "    for idx, id in enumerate(predicted):\n",
    "        if id in true:\n",
    "            hits += 1\n",
    "            score += hits / (idx + 1)\n",
    "    return score / len(true)\n",
    "\n",
    "\n",
    "def mean_average_precision(predictions_truths):\n",
    "    scores = [average_precision(pred, true) for pred, true in predictions_truths]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(predictions_truths):\n",
    "    total_rr = 0.0\n",
    "    for pred, true in predictions_truths:\n",
    "        for i, idx in enumerate(pred):\n",
    "            if idx in true:\n",
    "                total_rr += 1 / (i + 1)\n",
    "                break\n",
    "    return total_rr / len(predictions_truths)\n",
    "\n",
    "\n",
    "def test_retrieval(predictions_ground_truths, k):\n",
    "    for i, (pred, true) in enumerate(predictions_ground_truths):\n",
    "        p_at_k = precision_at_k(pred, true, k)\n",
    "        r_at_k = recall_at_k(pred, true, k)\n",
    "        ap = average_precision(pred, true)\n",
    "        print(f\"Query {i+1} - P@{k}: {p_at_k:.2f}, R@{k}: {r_at_k:.2f}, AP: {ap:.2f}\")\n",
    "\n",
    "    map_score = mean_average_precision(predictions_ground_truths)\n",
    "    mrr_score = mean_reciprocal_rank(predictions_ground_truths)\n",
    "\n",
    "    print(f\"\\nMAP: {map_score:.2f}\")\n",
    "    print(f\"MRR: {mrr_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fiqa\"\n",
    "data_path = \"./data/beir_data\" \n",
    "\n",
    "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
    "out_dir = util.download_and_unzip(url, data_path)\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(out_dir).load(split=\"test\")\n",
    "\n",
    "corpus_texts = [document['text'] for key, document in corpus.items()]\n",
    "corpus_keys = [key for key, document in corpus.items()]\n",
    "\n",
    "query_texts = [document for key, document in queries.items()]\n",
    "query_keys = [key for key, document in queries.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "\n",
    "k = 15\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizer.fit(corpus_texts + query_texts)\n",
    "corpus_tf_idf = vectorizer.transform(corpus_texts)\n",
    "query_tf_idf = vectorizer.transform(query_texts)\n",
    "\n",
    "cosine_similarities = corpus_tf_idf @ query_tf_idf.T\n",
    "\n",
    "predictions = []\n",
    "for cosine_similarity in cosine_similarities.T:\n",
    "    # top_k_indices = np.flip(np.argsort(cosine_similarity.toarray()).squeeze()[-k:]).tolist()\n",
    "    top_k_indices = cosine_similarity.toarray().argsort().squeeze()[::-1][:k].tolist()\n",
    "    predictions.append(top_k_indices)\n",
    "\n",
    "corpus_query_matches = {query_keys[query_idx]: [corpus_keys[idx] for idx in prediction] for query_idx, prediction in enumerate(predictions)}\n",
    "predictions_ground_truths = [(corpus_keys, list(qrels[query_key].keys()))for query_key, corpus_keys in corpus_query_matches.items()]\n",
    "\n",
    "test_retrieval(predictions_ground_truths, k=3)\n",
    "test_retrieval(predictions_ground_truths, k=5)\n",
    "test_retrieval(predictions_ground_truths, k=10)\n",
    "test_retrieval(predictions_ground_truths, k=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
