{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import shap\n",
    "\n",
    "# import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/netflix_stock_price/NFLX.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df.sort_values('Date', ascending=True)\n",
    "\n",
    "# df = df.drop(columns=['Adj Close', 'Volume'])\n",
    "\n",
    "date_column = 'Date'\n",
    "numerical_columns = df.columns.to_list()\n",
    "numerical_columns.remove(date_column)\n",
    "\n",
    "k = 3\n",
    "lower_quartile = df[numerical_columns].quantile(0.25)\n",
    "upper_quartile = df[numerical_columns].quantile(0.75)\n",
    "iqr = upper_quartile - lower_quartile\n",
    "for column in numerical_columns:\n",
    "    outlier_mask = (df[column] < lower_quartile[column] - k * iqr[column]) | (df[column] > upper_quartile[column] + k * iqr[column])\n",
    "    df = df[~outlier_mask]\n",
    "\n",
    "df[numerical_columns] = df[numerical_columns].astype(float)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure((30, 10))\n",
    "\n",
    "price_columns = numerical_columns[:]\n",
    "price_columns.remove('Volume')\n",
    "\n",
    "for column in price_columns:\n",
    "    sns.lineplot(df, x=date_column, y=column)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.ylabel('Normalized Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price_Range'] = df['High'] - df['Low']\n",
    "df['Daily_Return'] = df['Close'].pct_change()\n",
    "\n",
    "df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
    "df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "df['Momentum_5'] = df['Close'].diff(5)\n",
    "df['Momentum_10'] = df['Close'].diff(10)\n",
    "\n",
    "df['Volatility_5'] = df['Close'].rolling(window=5).std()\n",
    "df['Volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "delta = df['Close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# target_column = 'Close'\n",
    "feature_columns = df.columns.to_list()\n",
    "# feature_columns.remove(target_column)\n",
    "feature_columns.remove(date_column)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = {}\n",
    "# for column in feature_columns:\n",
    "#     scaler[column] = MinMaxScaler()\n",
    "#     df.loc[:, [column]] = scaler[column].fit_transform(df[[column]])\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "df.loc[:, feature_columns] = feature_scaler.fit_transform(df[feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.9\n",
    "sequence_length = 100\n",
    "num_samples = len(df[feature_columns].iloc[:-1].values)\n",
    "\n",
    "X = df[feature_columns].iloc[:-1].values\n",
    "y = df[feature_columns].iloc[1:].values\n",
    "\n",
    "num_sequences = len(X) // sequence_length\n",
    "\n",
    "X = X[:num_sequences * sequence_length]\n",
    "X = np.array_split(X, num_sequences, axis=0)\n",
    "X = np.array(X)\n",
    "\n",
    "y = y[:num_sequences * sequence_length]\n",
    "y = np.array_split(y, num_sequences, axis=0)\n",
    "y = np.array(y)\n",
    "\n",
    "# X.shape, y.shape\n",
    "\n",
    "train_size = int(train_fraction * num_sequences)\n",
    "\n",
    "indices = list(range(len(X)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "X_train = X[indices[:train_size]]\n",
    "y_train = y[indices[:train_size]]\n",
    "X_test = X[indices[train_size:]]\n",
    "y_test = y[indices[train_size:]]\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeSeriesLSTM(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_dim, output_dim, num_layers, hidden_dim, dropout):\n",
    "#         super(TimeSeriesLSTM, self).__init__()\n",
    "#         self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "#         # self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.lstm2 = nn.LSTM(2*hidden_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "#         # self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc1 = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm1(x)\n",
    "#         # out = self.bn1(out.transpose(1, 2)).transpose(1, 2)\n",
    "#         out = self.dropout1(out)\n",
    "        \n",
    "#         out, _ = self.lstm2(out)\n",
    "#         # out = self.bn2(out.transpose(1, 2)).transpose(1, 2)\n",
    "#         out = self.dropout2(out)  \n",
    "\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "class TimeSeriesLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_layers, hidden_dim, dropout):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = X_train.size(-1), X_train.size(-1)\n",
    "hidden_dim = 512\n",
    "num_layers = 4\n",
    "dropout = 0.4\n",
    "lr = 1e-3\n",
    "weight_decay = 3e-1\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# epochs = 1000\n",
    "# best_loss = float('inf')\n",
    "\n",
    "# model = TimeSeriesLSTM(\n",
    "#     input_dim=input_dim,\n",
    "#     output_dim=output_dim,\n",
    "#     num_layers=num_layers,\n",
    "#     hidden_dim=hidden_dim,\n",
    "#     dropout=dropout\n",
    "# )\n",
    "# model = model.to(device)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.AdamW(\n",
    "#     model.parameters(),\n",
    "#     lr=lr,\n",
    "#     weight_decay=weight_decay\n",
    "# )\n",
    "\n",
    "# # Initial test loss\n",
    "# model.eval()\n",
    "# test_loss = 0.0\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         y_pred = model(X_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         test_loss += loss.item()\n",
    "# print(f\"Initial Test Loss: {test_loss/len(test_loader):.6f}\")\n",
    "\n",
    "# # Training loop with early stopping+\n",
    "# progress = tqdm(range(epochs))\n",
    "# for epoch in progress:\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(X_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "    \n",
    "#     # Validation on test set\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch in test_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             test_loss += loss.item()\n",
    "#     avg_test_loss = test_loss / len(test_loader)\n",
    "#     avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "#     progress.set_description(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "#     # Save best model\n",
    "#     if avg_test_loss < best_loss:\n",
    "#         best_loss = avg_test_loss\n",
    "#         best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        \n",
    "# # Load best model\n",
    "# model = TimeSeriesLSTM(\n",
    "#     input_dim=input_dim,\n",
    "#     output_dim=output_dim,\n",
    "#     num_layers=num_layers,\n",
    "#     hidden_dim=hidden_dim,\n",
    "#     dropout=dropout\n",
    "# )\n",
    "# model.load_state_dict(best_model_state)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Best test loss\n",
    "# model.eval()\n",
    "# test_loss = 0.0\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         y_pred = model(X_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         test_loss += loss.item()\n",
    "# print(f\"Best Test Loss: {test_loss/len(test_loader):.6f}\")\n",
    "\n",
    "# torch.save(model.state_dict(), './data/netflix_stock_price/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesLSTM(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=num_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    dropout=dropout\n",
    ")\n",
    "state_dict = torch.load('./data/netflix_stock_price/best_model.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device)\n",
    "\n",
    "seed_sequences = 2\n",
    "pred_sequences = 2\n",
    "\n",
    "X_sample = torch.tensor(X[-(seed_sequences+pred_sequences):-pred_sequences].reshape(1, -1, input_dim).squeeze(), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(pred_sequences * sequence_length)):\n",
    "        y_pred = model(X_sample)[:, -1, :].unsqueeze(1)\n",
    "        X_sample = torch.cat((X_sample, y_pred), dim=1)\n",
    "\n",
    "preds = X_sample.detach().cpu().numpy().squeeze()\n",
    "\n",
    "# plt.plot(range(seed_sequences * sequence_length), preds[:seed_sequences * sequence_length], '--')\n",
    "# plt.plot(range(sequence_length * seed_sequences, len(preds)), preds[sequence_length * seed_sequences:], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_price_index = feature_columns.index('Close')\n",
    "\n",
    "obs = X[-pred_sequences:].reshape(-1, input_dim)\n",
    "\n",
    "rescaled_preds = feature_scaler.inverse_transform(preds)\n",
    "rescaled_obs = feature_scaler.inverse_transform(obs)\n",
    "\n",
    "closing_price_pred = rescaled_preds[:, [closing_price_index]]\n",
    "closing_price_obs = rescaled_obs[:, [closing_price_index]]\n",
    "\n",
    "plt.plot(range(seed_sequences * sequence_length), closing_price_pred[:seed_sequences * sequence_length], '--')\n",
    "plt.plot(range(sequence_length * seed_sequences, len(closing_price_pred)), closing_price_pred[sequence_length * seed_sequences:], 'r')\n",
    "plt.plot(range(sequence_length * seed_sequences, len(closing_price_pred)), closing_price_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# class Sequence(nn.Module):\n",
    "\n",
    "#     def __init__(self, hidden_dim=128, num_layers=2):\n",
    "#         super(Sequence, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "#         self.fully_connected = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         outputs, _ = self.lstm(inputs)\n",
    "#         outputs = self.fully_connected(outputs)\n",
    "#         return outputs\n",
    "\n",
    "#     def predict(self, inputs, steps):\n",
    "#         with torch.no_grad():\n",
    "#             for _ in range(steps):\n",
    "#                 output = self(inputs)[:, -1, :].unsqueeze(1)\n",
    "#                 inputs = torch.cat((inputs, output), dim=1)\n",
    "#         return inputs\n",
    "\n",
    "\n",
    "# epochs = 100\n",
    "# device = torch.device(\"mps\")\n",
    "\n",
    "# num_periods = 10\n",
    "# num_samples = 1000\n",
    "# # num_train_batches = 100\n",
    "# num_train_batches = 2\n",
    "\n",
    "# x = np.linspace(0, 2 * np.pi * num_periods, num_samples)\n",
    "# phi = np.random.rand(num_train_batches, 1) * 180 - 90\n",
    "# data = np.sin(x + phi)\n",
    "\n",
    "# train_inputs = torch.from_numpy(data[:, :-1]).float().to(device).unsqueeze(-1)\n",
    "# train_targets = torch.from_numpy(data[:, 1:]).float().to(device)\n",
    "# test_inputs = torch.from_numpy(data[:, :-1]).float().to(device).unsqueeze(-1)\n",
    "# test_targets = torch.from_numpy(data[:, 1:]).float().to(device)\n",
    "\n",
    "# model = Sequence().to(device)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# progress = tqdm(range(epochs))\n",
    "# for epoch in progress:\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(train_inputs)\n",
    "#     loss = criterion(outputs.squeeze(), train_targets)\n",
    "#     progress.set_description(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# seed_steps = 50\n",
    "# steps = 1000\n",
    "\n",
    "# preds = test_inputs.clone()[0, :seed_steps].unsqueeze(0).to(device)\n",
    "# with torch.no_grad():\n",
    "#     for _ in tqdm(range(steps)):\n",
    "#         output = model(preds)[:, -1, :].unsqueeze(1)\n",
    "#         preds = torch.cat((preds, output), dim=1)\n",
    "\n",
    "# preds = preds.detach().cpu().numpy().squeeze()\n",
    "\n",
    "# plt.plot(range(seed_steps), preds[:seed_steps], '--')\n",
    "# plt.plot(range(seed_steps, len(preds)), preds[seed_steps:], 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
