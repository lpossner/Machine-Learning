{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/housing/housing.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
    "categorical_columns = ['ocean_proximity']\n",
    "target_column = 'median_house_value'\n",
    "\n",
    "median_house_value_mask = df['median_house_value'] != df['median_house_value'].max()\n",
    "housing_median_age_mask = df['housing_median_age'] != df['housing_median_age'].max()\n",
    "\n",
    "df = df[(median_house_value_mask & housing_median_age_mask).to_list()]\n",
    "\n",
    "df[numerical_columns].hist(bins=50, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tukey_fences(series, k=1.5):\n",
    "#     q1 = series.quantile(0.25)\n",
    "#     q3 = series.quantile(0.75)\n",
    "#     iqr = q3 - q1\n",
    "#     lower_bound = q1 - k * iqr\n",
    "#     upper_bound = q3 + k * iqr\n",
    "#     outlier = (series < lower_bound) | (series > upper_bound)\n",
    "#     series[outlier] = series.median()\n",
    "#     return series\n",
    "\n",
    "\n",
    "# df.loc[:, numerical_columns] = df[numerical_columns].apply(tukey_fences, axis=0)\n",
    "\n",
    "df[categorical_columns] = df[categorical_columns].apply(pd.Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(objective='reg:squarederror', enable_categorical=True)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "print(\"Best model:\", best_model)\n",
    "\n",
    "r2 = best_model.score(X_test, y_test)\n",
    "print(\"Test R2:\", r2)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "feature_importance = best_model.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transformers = {}\n",
    "# transfomer_columns = numerical_columns + [target_column]\n",
    "# for column in transfomer_columns:\n",
    "#     transformer = QuantileTransformer(output_distribution='normal')\n",
    "#     df[column] = transformer.fit_transform(df[column].values.reshape(-1, 1))\n",
    "#     transformers[column] = transformer\n",
    "\n",
    "# df = pd.get_dummies(df, columns=categorical_columns).astype(float)\n",
    "\n",
    "# df[numerical_columns].hist(bins=50, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(columns=[target_column])\n",
    "# y = df[target_column]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, dropout_prob=0.1):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 1024),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(512, 256),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(256, 128),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(128, 64),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(64, 32),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(32, 16),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(16, 8),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(8, 4),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(4, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x).unsqueeze(-1)\n",
    "\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# model = MultiLayerPerceptron(input_dim=X_train.shape[1]).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500)\n",
    "\n",
    "# epochs = 5000\n",
    "# progress_bar = tqdm(range(epochs))\n",
    "# for epoch in progress_bar:\n",
    "#     model.train()\n",
    "#     epoch_loss = 0.0\n",
    "\n",
    "#     for X_batch, y_batch in dataloader:\n",
    "#         X_batch = X_batch.to(device)\n",
    "#         y_batch = y_batch.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(X_batch)\n",
    "#         loss = F.mse_loss(y_pred.squeeze(), y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item() * len(X_batch)\n",
    "\n",
    "#     epoch_loss /= len(dataloader.dataset)\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         y_pred = model(X_test_tensor.to(device))\n",
    "#         mae = F.l1_loss(y_pred.squeeze(), y_test_tensor.to(device)).item()\n",
    "#         mae_median_house_value = transformers[target_column].inverse_transform([[mae]])[0][0]\n",
    "\n",
    "#     progress_bar.set_description(f\"Loss: {epoch_loss:.4f}, MAE Median House Value: {mae_median_house_value:.4f}\")\n",
    "\n",
    "#     # scheduler.step(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv('./data/housing/housing.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "numerical_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
    "categorical_columns = ['ocean_proximity']\n",
    "target_column = 'median_house_value'\n",
    "\n",
    "median_house_value_mask = df['median_house_value'] != df['median_house_value'].max()\n",
    "housing_median_age_mask = df['housing_median_age'] != df['housing_median_age'].max()\n",
    "\n",
    "df = df[(median_house_value_mask & housing_median_age_mask).to_list()]\n",
    "\n",
    "# df = pd.get_dummies(df, columns=categorical_columns).astype(float)\n",
    "df[categorical_columns] = df[categorical_columns].apply(pd.Categorical)\n",
    "\n",
    "df[numerical_columns].hist(bins=50, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "pca = PCA()\n",
    "X[numerical_columns] = pca.fit_transform(X[numerical_columns])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(objective='reg:squarederror', enable_categorical=True)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "print(\"Best model:\", best_model)\n",
    "\n",
    "r2 = best_model.score(X_test, y_test)\n",
    "print(\"Test R2:\", r2)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
