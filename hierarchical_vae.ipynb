{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, z1_dim, z2_dim):\n",
    "        super(HVAE, self).__init__()\n",
    "        \n",
    "        # First Encoder (q(z1 | x))\n",
    "        self.e1_fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.e1_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.e1_fc_mu = nn.Linear(hidden_dim, z1_dim)\n",
    "        self.e1_fc_logvar = nn.Linear(hidden_dim, z1_dim)\n",
    "        \n",
    "        # Second Encoder (q(z2 | z1))\n",
    "        self.e2_fc1 = nn.Linear(z1_dim, hidden_dim)\n",
    "        self.e2_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.e2_fc_mu = nn.Linear(hidden_dim, z2_dim)\n",
    "        self.e2_fc_logvar = nn.Linear(hidden_dim, z2_dim)\n",
    "        \n",
    "        # First Decoder (p(z1 | z2))\n",
    "        self.d1_fc1 = nn.Linear(z2_dim, hidden_dim)\n",
    "        self.d1_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.d1_fc_mu = nn.Linear(hidden_dim, z1_dim)\n",
    "        self.d1_fc_logvar = nn.Linear(hidden_dim, z1_dim)\n",
    "        \n",
    "        # Second Decoder (p(x | z1))\n",
    "        self.d2_fc1 = nn.Linear(z1_dim, hidden_dim)\n",
    "        self.d2_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.d2_fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode_z1(self, x):\n",
    "        x = torch.relu(self.e1_fc1(x))\n",
    "        x = torch.relu(self.e1_fc2(x))\n",
    "        mu = self.e1_fc_mu(x)\n",
    "        logvar = self.e1_fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def encode_z2(self, x):\n",
    "        x = torch.relu(self.e2_fc1(x))\n",
    "        x = torch.relu(self.e2_fc2(x))\n",
    "        mu = self.e2_fc_mu(x)\n",
    "        logvar = self.e2_fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode_z1(self, x):\n",
    "        x = torch.relu(self.d1_fc1(x))\n",
    "        x = torch.relu(self.d1_fc2(x))\n",
    "        mu = self.d1_fc_mu(x)\n",
    "        logvar = self.d1_fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode_x(self, x):\n",
    "        x = torch.relu(self.d2_fc1(x))\n",
    "        x = torch.relu(self.d2_fc2(x))\n",
    "        return torch.sigmoid(self.d2_fc_out(x))\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     # Encode x to z1\n",
    "    #     mu_z1, logvar_z1 = self.encode_z1(x)\n",
    "    #     z1 = self.reparameterize(mu_z1, logvar_z1)\n",
    "        \n",
    "    #     # Encode z1 to z2\n",
    "    #     mu_z2, logvar_z2 = self.encode_z2(z1)\n",
    "    #     z2 = self.reparameterize(mu_z2, logvar_z2)\n",
    "        \n",
    "    #     # Decode z2 to z1\n",
    "    #     mu_z1_dec, logvar_z1_dec = self.decode_z1(z2)\n",
    "    #     z1_dec = self.reparameterize(mu_z1_dec, logvar_z1_dec)\n",
    "        \n",
    "    #     # Decode z1 to x\n",
    "    #     recon_x = self.decode_x(z1_dec)\n",
    "        \n",
    "    #     return recon_x, mu_z1, logvar_z1, mu_z2, logvar_z2, mu_z1_dec, logvar_z1_dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode x to z1\n",
    "        mu_z1, logvar_z1 = self.encode_z1(x)\n",
    "        z1 = self.reparameterize(mu_z1, logvar_z1)\n",
    "        \n",
    "        # # Encode z1 to z2\n",
    "        # mu_z2, logvar_z2 = self.encode_z2(z1)\n",
    "        # z2 = self.reparameterize(mu_z2, logvar_z2)\n",
    "        \n",
    "        # # Decode z2 to z1\n",
    "        # mu_z1_dec, logvar_z1_dec = self.decode_z1(z2)\n",
    "        # z1_dec = self.reparameterize(mu_z1_dec, logvar_z1_dec)\n",
    "        \n",
    "        # Decode z1 to x\n",
    "        # recon_x = self.decode_x(z1_dec)\n",
    "        recon_x = self.decode_x(z1)\n",
    "        \n",
    "        # return recon_x, mu_z1, logvar_z1, mu_z2, logvar_z2, mu_z1_dec, logvar_z1_dec\n",
    "        return recon_x, mu_z1, logvar_z1\n",
    "    \n",
    "\n",
    "# def loss_function(recon_x, x, mu_z1, logvar_z1, mu_z2, logvar_z2, mu_z1_dec, logvar_z1_dec):\n",
    "#     # Reconstruction loss\n",
    "#     BCE = nn.functional.binary_cross_entropy(recon_x, x.view(x.size(0), -1), reduction='sum')\n",
    "    \n",
    "#     # KL divergence for z1\n",
    "#     KLD_z1 = -0.5 * torch.sum(1 + logvar_z1 - mu_z1.pow(2) - logvar_z1.exp())\n",
    "    \n",
    "#     # KL divergence for z2\n",
    "#     KLD_z2 = -0.5 * torch.sum(1 + logvar_z2 - mu_z2.pow(2) - logvar_z2.exp())\n",
    "    \n",
    "#     # KL divergence for p(z1 | z2)\n",
    "#     KLD_z1_dec = -0.5 * torch.sum(1 + logvar_z1_dec - mu_z1_dec.pow(2) - logvar_z1_dec.exp())\n",
    "    \n",
    "#     return BCE + KLD_z1 + KLD_z2 + KLD_z1_dec\n",
    "\n",
    "def loss_function(recon_x, x, mu_z1, logvar_z1):\n",
    "    # Reconstruction loss\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(x.size(0), -1), reduction='sum')\n",
    "    \n",
    "    # # KL divergence for z1\n",
    "    # KLD_z1 = -0.5 * torch.sum(1 + logvar_z1 - mu_z1.pow(2) - logvar_z1.exp())\n",
    "    \n",
    "    # # KL divergence for z2\n",
    "    # KLD_z2 = -0.5 * torch.sum(1 + logvar_z2 - mu_z2.pow(2) - logvar_z2.exp())\n",
    "    \n",
    "    # KL divergence for p(z1 | z2)\n",
    "    # KLD_z1_dec = -0.5 * torch.sum(1 + logvar_z1_dec - mu_z1_dec.pow(2) - logvar_z1_dec.exp())\n",
    "    KLD_z1_dec = -0.5 * torch.sum(1 + logvar_z1 - mu_z1.pow(2) - logvar_z1.exp())\n",
    "    \n",
    "    # return BCE + KLD_z1 + KLD_z2 + KLD_z1_dec\n",
    "    return BCE + KLD_z1_dec\n",
    "\n",
    "\n",
    "# def train(model, train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for batch_idx, (data, _) in enumerate(train_loader):\n",
    "#         data = data.view(data.size(0), -1)\n",
    "#         optimizer.zero_grad()\n",
    "#         recon_batch, mu_z1, logvar_z1, mu_z2, logvar_z2, mu_z1_dec, logvar_z1_dec = model(data)\n",
    "#         loss = loss_function(recon_batch, data, mu_z1, logvar_z1, mu_z2, logvar_z2, mu_z1_dec, logvar_z1_dec)\n",
    "#         loss.backward()\n",
    "#         train_loss += loss.item()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print(f'Epoch {epoch}, Batch {batch_idx}, Loss {loss.item() / len(data):.6f}')\n",
    "    \n",
    "#     print(f'====> Epoch {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        # recon_batch, mu_z1, logvar_z1, mu_z2, logvar_z2, mu_z1_dec, logvar_z1_dec = model(data)\n",
    "        recon_batch, mu_z1, logvar_z1 = model(data)\n",
    "        # loss = loss_function(recon_batch, data, mu_z1, logvar_z1, mu_z2, logvar_z2, mu_z1_dec, logvar_z1_dec)\n",
    "        loss = loss_function(recon_batch, data, mu_z1, logvar_z1)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss {loss.item() / len(data):.6f}')\n",
    "    \n",
    "    print(f'====> Epoch {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale()\n",
    "])\n",
    "train_loader = DataLoader(\n",
    "    datasets.CelebA('../data', download=True, \n",
    "                    transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "input_dim  = 784\n",
    "hidden_dim = 512\n",
    "z1_dim = 256\n",
    "z2_dim = 128\n",
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale()\n",
    "])\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, \n",
    "                    transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, optimizer\n",
    "model = HVAE(input_dim=input_dim, hidden_dim=hidden_dim, z1_dim=z1_dim, z2_dim=z2_dim)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_size = 10\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z2 = torch.randn(plot_size, z2_dim)\n",
    "    mu_z1_dec, logvar_z1_dec = model.decode_z1(z2)\n",
    "    z1 = model.reparameterize(mu_z1_dec, logvar_z1_dec)\n",
    "    gen_x = model.decode_x(z1)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=plot_size, figsize=(10, 4))\n",
    "for idx in range(plot_size):\n",
    "    axes[idx].imshow(gen_x[idx].reshape(28, 28), cmap='gray')\n",
    "    axes[idx].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = \"\"\"\n",
    "Epoch 1, Batch 0, Loss 543.547607\n",
    "Epoch 1, Batch 100, Loss 200.759277\n",
    "Epoch 1, Batch 200, Loss 183.188370\n",
    "Epoch 1, Batch 300, Loss 174.097748\n",
    "Epoch 1, Batch 400, Loss 158.435394\n",
    "====> Epoch 1 Average loss: 186.8992\n",
    "Epoch 2, Batch 0, Loss 154.981644\n",
    "Epoch 2, Batch 100, Loss 153.991196\n",
    "Epoch 2, Batch 200, Loss 150.128845\n",
    "Epoch 2, Batch 300, Loss 141.944656\n",
    "Epoch 2, Batch 400, Loss 140.798309\n",
    "====> Epoch 2 Average loss: 146.7217\n",
    "Epoch 3, Batch 0, Loss 143.802765\n",
    "Epoch 3, Batch 100, Loss 130.242065\n",
    "Epoch 3, Batch 200, Loss 123.623161\n",
    "Epoch 3, Batch 300, Loss 124.204102\n",
    "Epoch 3, Batch 400, Loss 120.491562\n",
    "====> Epoch 3 Average loss: 128.1443\n",
    "Epoch 4, Batch 0, Loss 129.146133\n",
    "Epoch 4, Batch 100, Loss 118.330139\n",
    "Epoch 4, Batch 200, Loss 116.656929\n",
    "Epoch 4, Batch 300, Loss 116.649696\n",
    "Epoch 4, Batch 400, Loss 117.369095\n",
    "====> Epoch 4 Average loss: 118.9075\n",
    "Epoch 5, Batch 0, Loss 116.158661\n",
    "Epoch 5, Batch 100, Loss 114.048744\n",
    "Epoch 5, Batch 200, Loss 115.207306\n",
    "Epoch 5, Batch 300, Loss 111.927040\n",
    "Epoch 5, Batch 400, Loss 112.318794\n",
    "====> Epoch 5 Average loss: 113.7934\n",
    "Epoch 6, Batch 0, Loss 113.617569\n",
    "Epoch 6, Batch 100, Loss 106.008240\n",
    "Epoch 6, Batch 200, Loss 111.353745\n",
    "Epoch 6, Batch 300, Loss 112.988541\n",
    "Epoch 6, Batch 400, Loss 110.555664\n",
    "====> Epoch 6 Average loss: 111.2519\n",
    "Epoch 7, Batch 0, Loss 108.949440\n",
    "Epoch 7, Batch 100, Loss 108.506310\n",
    "Epoch 7, Batch 200, Loss 112.329124\n",
    "Epoch 7, Batch 300, Loss 110.276382\n",
    "Epoch 7, Batch 400, Loss 104.823639\n",
    "====> Epoch 7 Average loss: 109.4987\n",
    "Epoch 8, Batch 0, Loss 109.682922\n",
    "Epoch 8, Batch 100, Loss 105.976822\n",
    "Epoch 8, Batch 200, Loss 104.827728\n",
    "Epoch 8, Batch 300, Loss 104.897514\n",
    "Epoch 8, Batch 400, Loss 108.516060\n",
    "====> Epoch 8 Average loss: 108.3315\n",
    "Epoch 9, Batch 0, Loss 108.863556\n",
    "Epoch 9, Batch 100, Loss 105.741783\n",
    "Epoch 9, Batch 200, Loss 107.947037\n",
    "Epoch 9, Batch 300, Loss 105.026733\n",
    "Epoch 9, Batch 400, Loss 104.999474\n",
    "====> Epoch 9 Average loss: 107.3795\n",
    "Epoch 10, Batch 0, Loss 106.758118\n",
    "Epoch 10, Batch 100, Loss 102.404732\n",
    "Epoch 10, Batch 200, Loss 103.741447\n",
    "Epoch 10, Batch 300, Loss 104.335777\n",
    "Epoch 10, Batch 400, Loss 106.478851\n",
    "====> Epoch 10 Average loss: 106.5874\n",
    "Epoch 11, Batch 0, Loss 101.734848\n",
    "Epoch 11, Batch 100, Loss 106.094505\n",
    "Epoch 11, Batch 200, Loss 108.261688\n",
    "Epoch 11, Batch 300, Loss 103.133568\n",
    "Epoch 11, Batch 400, Loss 110.578522\n",
    "====> Epoch 11 Average loss: 105.7196\n",
    "Epoch 12, Batch 0, Loss 109.991463\n",
    "Epoch 12, Batch 100, Loss 105.537949\n",
    "Epoch 12, Batch 200, Loss 106.275558\n",
    "Epoch 12, Batch 300, Loss 99.426682\n",
    "Epoch 12, Batch 400, Loss 104.532478\n",
    "====> Epoch 12 Average loss: 104.8773\n",
    "Epoch 13, Batch 0, Loss 110.460823\n",
    "Epoch 13, Batch 100, Loss 100.695038\n",
    "Epoch 13, Batch 200, Loss 99.558548\n",
    "Epoch 13, Batch 300, Loss 107.786201\n",
    "Epoch 13, Batch 400, Loss 104.987167\n",
    "====> Epoch 13 Average loss: 104.3742\n",
    "Epoch 14, Batch 0, Loss 103.596664\n",
    "Epoch 14, Batch 100, Loss 104.036682\n",
    "Epoch 14, Batch 200, Loss 101.521713\n",
    "Epoch 14, Batch 300, Loss 105.920990\n",
    "Epoch 14, Batch 400, Loss 101.842667\n",
    "====> Epoch 14 Average loss: 103.9812\n",
    "Epoch 15, Batch 0, Loss 102.746162\n",
    "Epoch 15, Batch 100, Loss 105.038513\n",
    "Epoch 15, Batch 200, Loss 99.045273\n",
    "Epoch 15, Batch 300, Loss 101.360641\n",
    "Epoch 15, Batch 400, Loss 104.228050\n",
    "====> Epoch 15 Average loss: 103.5679\n",
    "Epoch 16, Batch 0, Loss 106.903587\n",
    "Epoch 16, Batch 100, Loss 103.620438\n",
    "Epoch 16, Batch 200, Loss 104.236816\n",
    "Epoch 16, Batch 300, Loss 104.630386\n",
    "Epoch 16, Batch 400, Loss 104.110229\n",
    "====> Epoch 16 Average loss: 103.2263\n",
    "Epoch 17, Batch 0, Loss 106.345840\n",
    "Epoch 17, Batch 100, Loss 101.753471\n",
    "Epoch 17, Batch 200, Loss 99.610168\n",
    "Epoch 17, Batch 300, Loss 102.387939\n",
    "Epoch 17, Batch 400, Loss 105.214058\n",
    "====> Epoch 17 Average loss: 102.8811\n",
    "Epoch 18, Batch 0, Loss 102.332451\n",
    "Epoch 18, Batch 100, Loss 103.479828\n",
    "Epoch 18, Batch 200, Loss 99.663406\n",
    "Epoch 18, Batch 300, Loss 104.315887\n",
    "Epoch 18, Batch 400, Loss 106.703125\n",
    "====> Epoch 18 Average loss: 102.6638\n",
    "Epoch 19, Batch 0, Loss 99.803650\n",
    "Epoch 19, Batch 100, Loss 101.219322\n",
    "Epoch 19, Batch 200, Loss 104.279182\n",
    "Epoch 19, Batch 300, Loss 104.647690\n",
    "Epoch 19, Batch 400, Loss 99.927361\n",
    "====> Epoch 19 Average loss: 102.3924\n",
    "Epoch 20, Batch 0, Loss 100.485161\n",
    "Epoch 20, Batch 100, Loss 100.669083\n",
    "Epoch 20, Batch 200, Loss 101.886375\n",
    "Epoch 20, Batch 300, Loss 98.304337\n",
    "Epoch 20, Batch 400, Loss 103.596436\n",
    "====> Epoch 20 Average loss: 101.9913\n",
    "Epoch 21, Batch 0, Loss 102.750473\n",
    "Epoch 21, Batch 100, Loss 106.238693\n",
    "Epoch 21, Batch 200, Loss 100.656067\n",
    "Epoch 21, Batch 300, Loss 100.271347\n",
    "Epoch 21, Batch 400, Loss 104.739517\n",
    "====> Epoch 21 Average loss: 101.7530\n",
    "Epoch 22, Batch 0, Loss 101.596931\n",
    "Epoch 22, Batch 100, Loss 99.276505\n",
    "Epoch 22, Batch 200, Loss 104.995880\n",
    "Epoch 22, Batch 300, Loss 104.220131\n",
    "Epoch 22, Batch 400, Loss 102.022842\n",
    "====> Epoch 22 Average loss: 101.4369\n",
    "Epoch 23, Batch 0, Loss 98.807266\n",
    "Epoch 23, Batch 100, Loss 99.702896\n",
    "Epoch 23, Batch 200, Loss 99.409317\n",
    "Epoch 23, Batch 300, Loss 102.568428\n",
    "Epoch 23, Batch 400, Loss 100.249969\n",
    "====> Epoch 23 Average loss: 101.2001\n",
    "Epoch 24, Batch 0, Loss 98.006302\n",
    "Epoch 24, Batch 100, Loss 99.825386\n",
    "Epoch 24, Batch 200, Loss 103.529213\n",
    "Epoch 24, Batch 300, Loss 100.456612\n",
    "Epoch 24, Batch 400, Loss 97.243782\n",
    "====> Epoch 24 Average loss: 101.0939\n",
    "Epoch 25, Batch 0, Loss 95.852257\n",
    "Epoch 25, Batch 100, Loss 99.511154\n",
    "Epoch 25, Batch 200, Loss 101.637115\n",
    "Epoch 25, Batch 300, Loss 100.100647\n",
    "Epoch 25, Batch 400, Loss 104.303391\n",
    "====> Epoch 25 Average loss: 100.8783\n",
    "Epoch 26, Batch 0, Loss 100.962311\n",
    "Epoch 26, Batch 100, Loss 98.581291\n",
    "Epoch 26, Batch 200, Loss 100.319313\n",
    "Epoch 26, Batch 300, Loss 98.792969\n",
    "Epoch 26, Batch 400, Loss 99.016663\n",
    "====> Epoch 26 Average loss: 100.6485\n",
    "Epoch 27, Batch 0, Loss 99.329376\n",
    "Epoch 27, Batch 100, Loss 101.407295\n",
    "Epoch 27, Batch 200, Loss 97.689743\n",
    "Epoch 27, Batch 300, Loss 100.244156\n",
    "Epoch 27, Batch 400, Loss 104.521782\n",
    "====> Epoch 27 Average loss: 100.5999\n",
    "Epoch 28, Batch 0, Loss 97.943726\n",
    "Epoch 28, Batch 100, Loss 102.897301\n",
    "Epoch 28, Batch 200, Loss 103.236252\n",
    "Epoch 28, Batch 300, Loss 98.896881\n",
    "Epoch 28, Batch 400, Loss 101.186920\n",
    "====> Epoch 28 Average loss: 100.4406\n",
    "Epoch 29, Batch 0, Loss 99.962051\n",
    "Epoch 29, Batch 100, Loss 100.918221\n",
    "Epoch 29, Batch 200, Loss 101.685303\n",
    "Epoch 29, Batch 300, Loss 98.329300\n",
    "Epoch 29, Batch 400, Loss 102.572533\n",
    "====> Epoch 29 Average loss: 100.3077\n",
    "Epoch 30, Batch 0, Loss 99.736801\n",
    "Epoch 30, Batch 100, Loss 99.687057\n",
    "Epoch 30, Batch 200, Loss 98.713318\n",
    "Epoch 30, Batch 300, Loss 99.444824\n",
    "Epoch 30, Batch 400, Loss 100.213936\n",
    "====> Epoch 30 Average loss: 100.1972\n",
    "Epoch 31, Batch 0, Loss 106.034653\n",
    "Epoch 31, Batch 100, Loss 96.214081\n",
    "Epoch 31, Batch 200, Loss 99.805260\n",
    "Epoch 31, Batch 300, Loss 103.848244\n",
    "Epoch 31, Batch 400, Loss 99.077492\n",
    "====> Epoch 31 Average loss: 100.0823\n",
    "Epoch 32, Batch 0, Loss 96.903732\n",
    "Epoch 32, Batch 100, Loss 100.423615\n",
    "Epoch 32, Batch 200, Loss 101.817772\n",
    "Epoch 32, Batch 300, Loss 99.186035\n",
    "Epoch 32, Batch 400, Loss 100.834045\n",
    "====> Epoch 32 Average loss: 99.9373\n",
    "Epoch 33, Batch 0, Loss 100.959702\n",
    "Epoch 33, Batch 100, Loss 103.477722\n",
    "Epoch 33, Batch 200, Loss 98.910233\n",
    "Epoch 33, Batch 300, Loss 98.964249\n",
    "Epoch 33, Batch 400, Loss 100.580948\n",
    "====> Epoch 33 Average loss: 99.8247\n",
    "Epoch 34, Batch 0, Loss 99.820862\n",
    "Epoch 34, Batch 100, Loss 99.006905\n",
    "Epoch 34, Batch 200, Loss 100.014526\n",
    "Epoch 34, Batch 300, Loss 97.082169\n",
    "Epoch 34, Batch 400, Loss 97.918030\n",
    "====> Epoch 34 Average loss: 99.6794\n",
    "Epoch 35, Batch 0, Loss 98.861923\n",
    "Epoch 35, Batch 100, Loss 102.023132\n",
    "Epoch 35, Batch 200, Loss 102.800934\n",
    "Epoch 35, Batch 300, Loss 99.324997\n",
    "Epoch 35, Batch 400, Loss 100.123444\n",
    "====> Epoch 35 Average loss: 99.6441\n",
    "Epoch 36, Batch 0, Loss 96.813133\n",
    "Epoch 36, Batch 100, Loss 101.529526\n",
    "Epoch 36, Batch 200, Loss 99.029869\n",
    "Epoch 36, Batch 300, Loss 100.989799\n",
    "Epoch 36, Batch 400, Loss 100.573433\n",
    "====> Epoch 36 Average loss: 99.6042\n",
    "Epoch 37, Batch 0, Loss 94.994186\n",
    "Epoch 37, Batch 100, Loss 96.370430\n",
    "Epoch 37, Batch 200, Loss 97.376907\n",
    "Epoch 37, Batch 300, Loss 102.132103\n",
    "Epoch 37, Batch 400, Loss 102.456345\n",
    "====> Epoch 37 Average loss: 99.4830\n",
    "Epoch 38, Batch 0, Loss 100.317177\n",
    "Epoch 38, Batch 100, Loss 100.051208\n",
    "Epoch 38, Batch 200, Loss 99.866913\n",
    "Epoch 38, Batch 300, Loss 99.945709\n",
    "Epoch 38, Batch 400, Loss 98.260132\n",
    "====> Epoch 38 Average loss: 99.4126\n",
    "Epoch 39, Batch 0, Loss 99.032951\n",
    "Epoch 39, Batch 100, Loss 99.208710\n",
    "Epoch 39, Batch 200, Loss 98.606300\n",
    "Epoch 39, Batch 300, Loss 100.621704\n",
    "Epoch 39, Batch 400, Loss 99.247726\n",
    "====> Epoch 39 Average loss: 99.3440\n",
    "Epoch 40, Batch 0, Loss 101.100464\n",
    "Epoch 40, Batch 100, Loss 97.913925\n",
    "Epoch 40, Batch 200, Loss 100.120811\n",
    "Epoch 40, Batch 300, Loss 101.931152\n",
    "Epoch 40, Batch 400, Loss 98.191895\n",
    "====> Epoch 40 Average loss: 99.1829\n",
    "Epoch 41, Batch 0, Loss 102.022972\n",
    "Epoch 41, Batch 100, Loss 99.536751\n",
    "Epoch 41, Batch 200, Loss 100.649147\n",
    "Epoch 41, Batch 300, Loss 98.840668\n",
    "Epoch 41, Batch 400, Loss 99.878784\n",
    "====> Epoch 41 Average loss: 99.1876\n",
    "Epoch 42, Batch 0, Loss 100.331474\n",
    "Epoch 42, Batch 100, Loss 96.528206\n",
    "Epoch 42, Batch 200, Loss 99.615837\n",
    "Epoch 42, Batch 300, Loss 96.659035\n",
    "Epoch 42, Batch 400, Loss 99.469185\n",
    "====> Epoch 42 Average loss: 99.0982\n",
    "Epoch 43, Batch 0, Loss 96.912041\n",
    "Epoch 43, Batch 100, Loss 99.166283\n",
    "Epoch 43, Batch 200, Loss 102.121254\n",
    "Epoch 43, Batch 300, Loss 104.742844\n",
    "Epoch 43, Batch 400, Loss 100.310005\n",
    "====> Epoch 43 Average loss: 99.0061\n",
    "Epoch 44, Batch 0, Loss 98.161545\n",
    "Epoch 44, Batch 100, Loss 100.308853\n",
    "Epoch 44, Batch 200, Loss 101.528114\n",
    "Epoch 44, Batch 300, Loss 99.019119\n",
    "Epoch 44, Batch 400, Loss 97.937302\n",
    "====> Epoch 44 Average loss: 98.9629\n",
    "Epoch 45, Batch 0, Loss 98.516464\n",
    "Epoch 45, Batch 100, Loss 99.266487\n",
    "Epoch 45, Batch 200, Loss 98.691154\n",
    "Epoch 45, Batch 300, Loss 106.046410\n",
    "Epoch 45, Batch 400, Loss 96.517395\n",
    "====> Epoch 45 Average loss: 98.8446\n",
    "Epoch 46, Batch 0, Loss 101.493683\n",
    "Epoch 46, Batch 100, Loss 96.825958\n",
    "Epoch 46, Batch 200, Loss 95.895752\n",
    "Epoch 46, Batch 300, Loss 102.287193\n",
    "Epoch 46, Batch 400, Loss 99.529167\n",
    "====> Epoch 46 Average loss: 98.7670\n",
    "Epoch 47, Batch 0, Loss 100.374748\n",
    "Epoch 47, Batch 100, Loss 101.656578\n",
    "Epoch 47, Batch 200, Loss 100.496384\n",
    "Epoch 47, Batch 300, Loss 98.394424\n",
    "Epoch 47, Batch 400, Loss 99.109978\n",
    "====> Epoch 47 Average loss: 98.7285\n",
    "Epoch 48, Batch 0, Loss 102.922363\n",
    "Epoch 48, Batch 100, Loss 99.732018\n",
    "Epoch 48, Batch 200, Loss 98.647964\n",
    "Epoch 48, Batch 300, Loss 99.990875\n",
    "Epoch 48, Batch 400, Loss 97.957214\n",
    "====> Epoch 48 Average loss: 98.6530\n",
    "Epoch 49, Batch 0, Loss 94.070442\n",
    "Epoch 49, Batch 100, Loss 99.038635\n",
    "Epoch 49, Batch 200, Loss 101.709747\n",
    "Epoch 49, Batch 300, Loss 97.134041\n",
    "Epoch 49, Batch 400, Loss 95.813461\n",
    "====> Epoch 49 Average loss: 98.6251\n",
    "Epoch 50, Batch 0, Loss 93.040848\n",
    "Epoch 50, Batch 100, Loss 97.278656\n",
    "Epoch 50, Batch 200, Loss 91.077744\n",
    "Epoch 50, Batch 300, Loss 97.315292\n",
    "Epoch 50, Batch 400, Loss 97.825363\n",
    "====> Epoch 50 Average loss: 98.5804\n",
    "Epoch 51, Batch 0, Loss 98.220169\n",
    "Epoch 51, Batch 100, Loss 98.089157\n",
    "Epoch 51, Batch 200, Loss 98.749191\n",
    "Epoch 51, Batch 300, Loss 99.589348\n",
    "Epoch 51, Batch 400, Loss 95.425255\n",
    "====> Epoch 51 Average loss: 98.4278\n",
    "Epoch 52, Batch 0, Loss 100.800735\n",
    "Epoch 52, Batch 100, Loss 99.175087\n",
    "Epoch 52, Batch 200, Loss 97.373024\n",
    "Epoch 52, Batch 300, Loss 102.045364\n",
    "Epoch 52, Batch 400, Loss 102.572899\n",
    "====> Epoch 52 Average loss: 98.3987\n",
    "Epoch 53, Batch 0, Loss 101.002914\n",
    "Epoch 53, Batch 100, Loss 99.218788\n",
    "Epoch 53, Batch 200, Loss 97.021530\n",
    "Epoch 53, Batch 300, Loss 95.688858\n",
    "Epoch 53, Batch 400, Loss 98.904327\n",
    "====> Epoch 53 Average loss: 98.3268\n",
    "Epoch 54, Batch 0, Loss 98.549561\n",
    "Epoch 54, Batch 100, Loss 96.881485\n",
    "Epoch 54, Batch 200, Loss 95.830376\n",
    "Epoch 54, Batch 300, Loss 99.715118\n",
    "Epoch 54, Batch 400, Loss 99.083511\n",
    "====> Epoch 54 Average loss: 98.2981\n",
    "Epoch 55, Batch 0, Loss 97.872032\n",
    "Epoch 55, Batch 100, Loss 99.000275\n",
    "Epoch 55, Batch 200, Loss 101.753510\n",
    "Epoch 55, Batch 300, Loss 96.707138\n",
    "Epoch 55, Batch 400, Loss 99.928726\n",
    "====> Epoch 55 Average loss: 98.2205\n",
    "Epoch 56, Batch 0, Loss 96.668091\n",
    "Epoch 56, Batch 100, Loss 100.518135\n",
    "Epoch 56, Batch 200, Loss 100.563019\n",
    "Epoch 56, Batch 300, Loss 100.225487\n",
    "Epoch 56, Batch 400, Loss 99.511566\n",
    "====> Epoch 56 Average loss: 98.1336\n",
    "Epoch 57, Batch 0, Loss 105.182930\n",
    "Epoch 57, Batch 100, Loss 98.773888\n",
    "Epoch 57, Batch 200, Loss 97.586075\n",
    "Epoch 57, Batch 300, Loss 101.463646\n",
    "Epoch 57, Batch 400, Loss 104.873383\n",
    "====> Epoch 57 Average loss: 98.1330\n",
    "Epoch 58, Batch 0, Loss 97.944565\n",
    "Epoch 58, Batch 100, Loss 98.119415\n",
    "Epoch 58, Batch 200, Loss 99.528893\n",
    "Epoch 58, Batch 300, Loss 99.624519\n",
    "Epoch 58, Batch 400, Loss 101.399055\n",
    "====> Epoch 58 Average loss: 98.1104\n",
    "Epoch 59, Batch 0, Loss 98.502571\n",
    "Epoch 59, Batch 100, Loss 103.021431\n",
    "Epoch 59, Batch 200, Loss 101.887802\n",
    "Epoch 59, Batch 300, Loss 94.880211\n",
    "Epoch 59, Batch 400, Loss 98.248962\n",
    "====> Epoch 59 Average loss: 98.0358\n",
    "Epoch 60, Batch 0, Loss 95.892570\n",
    "Epoch 60, Batch 100, Loss 102.980095\n",
    "Epoch 60, Batch 200, Loss 95.674652\n",
    "Epoch 60, Batch 300, Loss 94.307938\n",
    "Epoch 60, Batch 400, Loss 95.566368\n",
    "====> Epoch 60 Average loss: 97.9519\n",
    "Epoch 61, Batch 0, Loss 95.745186\n",
    "Epoch 61, Batch 100, Loss 96.157990\n",
    "Epoch 61, Batch 200, Loss 98.601578\n",
    "Epoch 61, Batch 300, Loss 95.265724\n",
    "Epoch 61, Batch 400, Loss 97.369675\n",
    "====> Epoch 61 Average loss: 97.9392\n",
    "Epoch 62, Batch 0, Loss 100.416496\n",
    "Epoch 62, Batch 100, Loss 95.616119\n",
    "Epoch 62, Batch 200, Loss 99.639427\n",
    "Epoch 62, Batch 300, Loss 97.183350\n",
    "Epoch 62, Batch 400, Loss 98.669556\n",
    "====> Epoch 62 Average loss: 97.9028\n",
    "Epoch 63, Batch 0, Loss 95.087936\n",
    "Epoch 63, Batch 100, Loss 102.260529\n",
    "Epoch 63, Batch 200, Loss 101.106415\n",
    "Epoch 63, Batch 300, Loss 96.876968\n",
    "Epoch 63, Batch 400, Loss 95.754959\n",
    "====> Epoch 63 Average loss: 97.8861\n",
    "Epoch 64, Batch 0, Loss 95.523529\n",
    "Epoch 64, Batch 100, Loss 97.710754\n",
    "Epoch 64, Batch 200, Loss 93.059166\n",
    "Epoch 64, Batch 300, Loss 96.487724\n",
    "Epoch 64, Batch 400, Loss 98.017418\n",
    "====> Epoch 64 Average loss: 97.8171\n",
    "Epoch 65, Batch 0, Loss 98.261024\n",
    "Epoch 65, Batch 100, Loss 97.801483\n",
    "Epoch 65, Batch 200, Loss 94.966064\n",
    "Epoch 65, Batch 300, Loss 96.314873\n",
    "Epoch 65, Batch 400, Loss 92.995361\n",
    "====> Epoch 65 Average loss: 97.7444\n",
    "Epoch 66, Batch 0, Loss 97.291504\n",
    "Epoch 66, Batch 100, Loss 98.537003\n",
    "Epoch 66, Batch 200, Loss 97.258171\n",
    "Epoch 66, Batch 300, Loss 100.736282\n",
    "Epoch 66, Batch 400, Loss 95.405403\n",
    "====> Epoch 66 Average loss: 97.7313\n",
    "Epoch 67, Batch 0, Loss 99.065781\n",
    "Epoch 67, Batch 100, Loss 101.627792\n",
    "Epoch 67, Batch 200, Loss 99.459335\n",
    "Epoch 67, Batch 300, Loss 96.077599\n",
    "Epoch 67, Batch 400, Loss 95.549568\n",
    "====> Epoch 67 Average loss: 97.6447\n",
    "Epoch 68, Batch 0, Loss 93.946518\n",
    "Epoch 68, Batch 100, Loss 100.235008\n",
    "Epoch 68, Batch 200, Loss 98.704269\n",
    "Epoch 68, Batch 300, Loss 99.016479\n",
    "Epoch 68, Batch 400, Loss 95.403931\n",
    "====> Epoch 68 Average loss: 97.6575\n",
    "Epoch 69, Batch 0, Loss 97.974701\n",
    "Epoch 69, Batch 100, Loss 97.249466\n",
    "Epoch 69, Batch 200, Loss 96.736443\n",
    "Epoch 69, Batch 300, Loss 96.043083\n",
    "Epoch 69, Batch 400, Loss 94.110687\n",
    "====> Epoch 69 Average loss: 97.6225\n",
    "Epoch 70, Batch 0, Loss 98.329880\n",
    "Epoch 70, Batch 100, Loss 100.807358\n",
    "Epoch 70, Batch 200, Loss 97.062698\n",
    "Epoch 70, Batch 300, Loss 101.261719\n",
    "Epoch 70, Batch 400, Loss 95.263412\n",
    "====> Epoch 70 Average loss: 97.5661\n",
    "Epoch 71, Batch 0, Loss 95.667862\n",
    "Epoch 71, Batch 100, Loss 98.110008\n",
    "Epoch 71, Batch 200, Loss 97.087959\n",
    "Epoch 71, Batch 300, Loss 95.726410\n",
    "Epoch 71, Batch 400, Loss 100.007614\n",
    "====> Epoch 71 Average loss: 97.4856\n",
    "Epoch 72, Batch 0, Loss 98.366966\n",
    "Epoch 72, Batch 100, Loss 92.865570\n",
    "Epoch 72, Batch 200, Loss 99.935760\n",
    "Epoch 72, Batch 300, Loss 99.417099\n",
    "Epoch 72, Batch 400, Loss 96.708008\n",
    "====> Epoch 72 Average loss: 97.5596\n",
    "Epoch 73, Batch 0, Loss 95.313828\n",
    "Epoch 73, Batch 100, Loss 97.733490\n",
    "Epoch 73, Batch 200, Loss 96.858574\n",
    "Epoch 73, Batch 300, Loss 102.821892\n",
    "Epoch 73, Batch 400, Loss 96.562973\n",
    "====> Epoch 73 Average loss: 97.4643\n",
    "Epoch 74, Batch 0, Loss 96.582596\n",
    "Epoch 74, Batch 100, Loss 97.174240\n",
    "Epoch 74, Batch 200, Loss 96.710793\n",
    "Epoch 74, Batch 300, Loss 96.890648\n",
    "Epoch 74, Batch 400, Loss 96.013344\n",
    "====> Epoch 74 Average loss: 97.4554\n",
    "Epoch 75, Batch 0, Loss 96.288773\n",
    "Epoch 75, Batch 100, Loss 97.322952\n",
    "Epoch 75, Batch 200, Loss 98.562561\n",
    "Epoch 75, Batch 300, Loss 98.249016\n",
    "Epoch 75, Batch 400, Loss 100.687614\n",
    "====> Epoch 75 Average loss: 97.3871\n",
    "Epoch 76, Batch 0, Loss 101.927872\n",
    "Epoch 76, Batch 100, Loss 97.044701\n",
    "Epoch 76, Batch 200, Loss 91.253853\n",
    "Epoch 76, Batch 300, Loss 99.426025\n",
    "Epoch 76, Batch 400, Loss 95.911461\n",
    "====> Epoch 76 Average loss: 97.4161\n",
    "Epoch 77, Batch 0, Loss 98.048172\n",
    "Epoch 77, Batch 100, Loss 94.952980\n",
    "Epoch 77, Batch 200, Loss 97.823196\n",
    "Epoch 77, Batch 300, Loss 97.677673\n",
    "Epoch 77, Batch 400, Loss 100.926697\n",
    "====> Epoch 77 Average loss: 97.3702\n",
    "Epoch 78, Batch 0, Loss 96.178810\n",
    "Epoch 78, Batch 100, Loss 96.646347\n",
    "Epoch 78, Batch 200, Loss 101.038498\n",
    "Epoch 78, Batch 300, Loss 94.719650\n",
    "Epoch 78, Batch 400, Loss 99.802643\n",
    "====> Epoch 78 Average loss: 97.3482\n",
    "Epoch 79, Batch 0, Loss 94.483803\n",
    "Epoch 79, Batch 100, Loss 97.788963\n",
    "Epoch 79, Batch 200, Loss 96.682739\n",
    "Epoch 79, Batch 300, Loss 96.648346\n",
    "Epoch 79, Batch 400, Loss 97.582359\n",
    "====> Epoch 79 Average loss: 97.2904\n",
    "Epoch 80, Batch 0, Loss 95.704033\n",
    "Epoch 80, Batch 100, Loss 98.233597\n",
    "Epoch 80, Batch 200, Loss 101.300903\n",
    "Epoch 80, Batch 300, Loss 97.370636\n",
    "Epoch 80, Batch 400, Loss 98.013786\n",
    "====> Epoch 80 Average loss: 97.2800\n",
    "Epoch 81, Batch 0, Loss 99.353165\n",
    "Epoch 81, Batch 100, Loss 99.689209\n",
    "Epoch 81, Batch 200, Loss 98.410057\n",
    "Epoch 81, Batch 300, Loss 94.383530\n",
    "Epoch 81, Batch 400, Loss 96.675171\n",
    "====> Epoch 81 Average loss: 97.2402\n",
    "Epoch 82, Batch 0, Loss 97.581810\n",
    "Epoch 82, Batch 100, Loss 96.384048\n",
    "Epoch 82, Batch 200, Loss 98.251503\n",
    "Epoch 82, Batch 300, Loss 98.438545\n",
    "Epoch 82, Batch 400, Loss 103.521645\n",
    "====> Epoch 82 Average loss: 97.2142\n",
    "Epoch 83, Batch 0, Loss 92.610901\n",
    "Epoch 83, Batch 100, Loss 95.828026\n",
    "Epoch 83, Batch 200, Loss 96.908043\n",
    "Epoch 83, Batch 300, Loss 97.484512\n",
    "Epoch 83, Batch 400, Loss 97.236954\n",
    "====> Epoch 83 Average loss: 97.1489\n",
    "Epoch 84, Batch 0, Loss 97.247147\n",
    "Epoch 84, Batch 100, Loss 94.091728\n",
    "Epoch 84, Batch 200, Loss 99.098557\n",
    "Epoch 84, Batch 300, Loss 97.673309\n",
    "Epoch 84, Batch 400, Loss 95.708176\n",
    "====> Epoch 84 Average loss: 97.1291\n",
    "Epoch 85, Batch 0, Loss 94.454300\n",
    "Epoch 85, Batch 100, Loss 96.149963\n",
    "Epoch 85, Batch 200, Loss 98.107674\n",
    "Epoch 85, Batch 300, Loss 100.159073\n",
    "Epoch 85, Batch 400, Loss 92.655914\n",
    "====> Epoch 85 Average loss: 97.0950\n",
    "Epoch 86, Batch 0, Loss 96.384521\n",
    "Epoch 86, Batch 100, Loss 93.393616\n",
    "Epoch 86, Batch 200, Loss 97.470337\n",
    "Epoch 86, Batch 300, Loss 95.886253\n",
    "Epoch 86, Batch 400, Loss 93.887367\n",
    "====> Epoch 86 Average loss: 97.1260\n",
    "Epoch 87, Batch 0, Loss 98.931747\n",
    "Epoch 87, Batch 100, Loss 99.747772\n",
    "Epoch 87, Batch 200, Loss 97.122879\n",
    "Epoch 87, Batch 300, Loss 96.938622\n",
    "Epoch 87, Batch 400, Loss 97.013168\n",
    "====> Epoch 87 Average loss: 97.0731\n",
    "Epoch 88, Batch 0, Loss 96.940208\n",
    "Epoch 88, Batch 100, Loss 97.310791\n",
    "Epoch 88, Batch 200, Loss 95.109375\n",
    "Epoch 88, Batch 300, Loss 91.415558\n",
    "Epoch 88, Batch 400, Loss 99.515396\n",
    "====> Epoch 88 Average loss: 97.0842\n",
    "Epoch 89, Batch 0, Loss 97.970573\n",
    "Epoch 89, Batch 100, Loss 101.554024\n",
    "Epoch 89, Batch 200, Loss 95.531105\n",
    "Epoch 89, Batch 300, Loss 95.519943\n",
    "Epoch 89, Batch 400, Loss 96.345230\n",
    "====> Epoch 89 Average loss: 96.9823\n",
    "Epoch 90, Batch 0, Loss 94.990166\n",
    "Epoch 90, Batch 100, Loss 98.783791\n",
    "Epoch 90, Batch 200, Loss 98.412262\n",
    "Epoch 90, Batch 300, Loss 94.263184\n",
    "Epoch 90, Batch 400, Loss 96.113228\n",
    "====> Epoch 90 Average loss: 97.0530\n",
    "Epoch 91, Batch 0, Loss 97.226410\n",
    "Epoch 91, Batch 100, Loss 99.137169\n",
    "Epoch 91, Batch 200, Loss 95.312927\n",
    "Epoch 91, Batch 300, Loss 97.272667\n",
    "Epoch 91, Batch 400, Loss 93.482704\n",
    "====> Epoch 91 Average loss: 96.9971\n",
    "Epoch 92, Batch 0, Loss 98.748810\n",
    "Epoch 92, Batch 100, Loss 96.142136\n",
    "Epoch 92, Batch 200, Loss 99.311981\n",
    "Epoch 92, Batch 300, Loss 96.147415\n",
    "Epoch 92, Batch 400, Loss 93.798218\n",
    "====> Epoch 92 Average loss: 96.9901\n",
    "Epoch 93, Batch 0, Loss 98.360176\n",
    "Epoch 93, Batch 100, Loss 97.680550\n",
    "Epoch 93, Batch 200, Loss 95.961426\n",
    "Epoch 93, Batch 300, Loss 95.249443\n",
    "Epoch 93, Batch 400, Loss 95.823303\n",
    "====> Epoch 93 Average loss: 96.9325\n",
    "Epoch 94, Batch 0, Loss 94.478806\n",
    "Epoch 94, Batch 100, Loss 94.684952\n",
    "Epoch 94, Batch 200, Loss 98.631691\n",
    "Epoch 94, Batch 300, Loss 91.030075\n",
    "Epoch 94, Batch 400, Loss 96.788048\n",
    "====> Epoch 94 Average loss: 96.9180\n",
    "Epoch 95, Batch 0, Loss 94.098518\n",
    "Epoch 95, Batch 100, Loss 99.391968\n",
    "Epoch 95, Batch 200, Loss 95.999786\n",
    "Epoch 95, Batch 300, Loss 95.077805\n",
    "Epoch 95, Batch 400, Loss 97.256447\n",
    "====> Epoch 95 Average loss: 96.8981\n",
    "Epoch 96, Batch 0, Loss 98.921906\n",
    "Epoch 96, Batch 100, Loss 93.073578\n",
    "Epoch 96, Batch 200, Loss 97.333374\n",
    "Epoch 96, Batch 300, Loss 95.763458\n",
    "Epoch 96, Batch 400, Loss 95.009521\n",
    "====> Epoch 96 Average loss: 96.8675\n",
    "Epoch 97, Batch 0, Loss 99.966179\n",
    "Epoch 97, Batch 100, Loss 96.358238\n",
    "Epoch 97, Batch 200, Loss 97.880348\n",
    "Epoch 97, Batch 300, Loss 95.035652\n",
    "Epoch 97, Batch 400, Loss 97.970383\n",
    "====> Epoch 97 Average loss: 96.9024\n",
    "Epoch 98, Batch 0, Loss 100.041550\n",
    "Epoch 98, Batch 100, Loss 97.245590\n",
    "Epoch 98, Batch 200, Loss 95.238983\n",
    "Epoch 98, Batch 300, Loss 98.799255\n",
    "Epoch 98, Batch 400, Loss 95.503265\n",
    "====> Epoch 98 Average loss: 96.8087\n",
    "Epoch 99, Batch 0, Loss 96.872986\n",
    "Epoch 99, Batch 100, Loss 95.061340\n",
    "Epoch 99, Batch 200, Loss 97.318123\n",
    "Epoch 99, Batch 300, Loss 98.121185\n",
    "Epoch 99, Batch 400, Loss 99.309372\n",
    "====> Epoch 99 Average loss: 96.8362\n",
    "Epoch 100, Batch 0, Loss 98.381073\n",
    "Epoch 100, Batch 100, Loss 96.903519\n",
    "Epoch 100, Batch 200, Loss 97.896729\n",
    "Epoch 100, Batch 300, Loss 96.008759\n",
    "Epoch 100, Batch 400, Loss 98.371147\n",
    "====> Epoch 100 Average loss: 96.7339\n",
    "Epoch 101, Batch 0, Loss 95.824066\n",
    "Epoch 101, Batch 100, Loss 99.698112\n",
    "Epoch 101, Batch 200, Loss 95.031555\n",
    "Epoch 101, Batch 300, Loss 97.615891\n",
    "Epoch 101, Batch 400, Loss 96.849564\n",
    "====> Epoch 101 Average loss: 96.7680\n",
    "Epoch 102, Batch 0, Loss 94.044945\n",
    "Epoch 102, Batch 100, Loss 100.005409\n",
    "Epoch 102, Batch 200, Loss 97.908203\n",
    "Epoch 102, Batch 300, Loss 94.550674\n",
    "Epoch 102, Batch 400, Loss 98.247818\n",
    "====> Epoch 102 Average loss: 96.7864\n",
    "Epoch 103, Batch 0, Loss 95.477020\n",
    "Epoch 103, Batch 100, Loss 95.349083\n",
    "Epoch 103, Batch 200, Loss 93.625656\n",
    "Epoch 103, Batch 300, Loss 97.224152\n",
    "Epoch 103, Batch 400, Loss 92.725006\n",
    "====> Epoch 103 Average loss: 96.7347\n",
    "Epoch 104, Batch 0, Loss 94.391220\n",
    "Epoch 104, Batch 100, Loss 96.676437\n",
    "Epoch 104, Batch 200, Loss 97.556625\n",
    "Epoch 104, Batch 300, Loss 94.064392\n",
    "Epoch 104, Batch 400, Loss 94.092293\n",
    "====> Epoch 104 Average loss: 96.7281\n",
    "Epoch 105, Batch 0, Loss 91.590134\n",
    "Epoch 105, Batch 100, Loss 94.542336\n",
    "Epoch 105, Batch 200, Loss 98.706711\n",
    "Epoch 105, Batch 300, Loss 98.116699\n",
    "Epoch 105, Batch 400, Loss 100.325516\n",
    "====> Epoch 105 Average loss: 96.7124\n",
    "Epoch 106, Batch 0, Loss 99.119926\n",
    "Epoch 106, Batch 100, Loss 97.517685\n",
    "Epoch 106, Batch 200, Loss 97.404594\n",
    "Epoch 106, Batch 300, Loss 93.680908\n",
    "Epoch 106, Batch 400, Loss 95.093033\n",
    "====> Epoch 106 Average loss: 96.6885\n",
    "Epoch 107, Batch 0, Loss 98.019066\n",
    "Epoch 107, Batch 100, Loss 99.200371\n",
    "Epoch 107, Batch 200, Loss 95.567352\n",
    "Epoch 107, Batch 300, Loss 93.834099\n",
    "Epoch 107, Batch 400, Loss 98.901360\n",
    "====> Epoch 107 Average loss: 96.6173\n",
    "Epoch 108, Batch 0, Loss 98.483543\n",
    "Epoch 108, Batch 100, Loss 95.724541\n",
    "Epoch 108, Batch 200, Loss 96.210121\n",
    "Epoch 108, Batch 300, Loss 95.799133\n",
    "Epoch 108, Batch 400, Loss 96.816132\n",
    "====> Epoch 108 Average loss: 96.6350\n",
    "Epoch 109, Batch 0, Loss 95.868256\n",
    "Epoch 109, Batch 100, Loss 96.264046\n",
    "Epoch 109, Batch 200, Loss 98.595428\n",
    "Epoch 109, Batch 300, Loss 94.740067\n",
    "Epoch 109, Batch 400, Loss 97.974503\n",
    "====> Epoch 109 Average loss: 96.5978\n",
    "Epoch 110, Batch 0, Loss 99.329140\n",
    "Epoch 110, Batch 100, Loss 97.030853\n",
    "Epoch 110, Batch 200, Loss 93.205696\n",
    "Epoch 110, Batch 300, Loss 93.888290\n",
    "Epoch 110, Batch 400, Loss 94.541191\n",
    "====> Epoch 110 Average loss: 96.5610\n",
    "Epoch 111, Batch 0, Loss 96.543602\n",
    "Epoch 111, Batch 100, Loss 98.289619\n",
    "Epoch 111, Batch 200, Loss 95.891602\n",
    "Epoch 111, Batch 300, Loss 97.303261\n",
    "Epoch 111, Batch 400, Loss 96.293976\n",
    "====> Epoch 111 Average loss: 96.5834\n",
    "Epoch 112, Batch 0, Loss 99.362167\n",
    "Epoch 112, Batch 100, Loss 96.167725\n",
    "Epoch 112, Batch 200, Loss 94.402985\n",
    "Epoch 112, Batch 300, Loss 98.153915\n",
    "Epoch 112, Batch 400, Loss 98.523056\n",
    "====> Epoch 112 Average loss: 96.5284\n",
    "Epoch 113, Batch 0, Loss 97.044258\n",
    "Epoch 113, Batch 100, Loss 95.957222\n",
    "Epoch 113, Batch 200, Loss 96.268707\n",
    "Epoch 113, Batch 300, Loss 101.134033\n",
    "Epoch 113, Batch 400, Loss 93.882240\n",
    "====> Epoch 113 Average loss: 96.5414\n",
    "Epoch 114, Batch 0, Loss 93.173828\n",
    "Epoch 114, Batch 100, Loss 96.515869\n",
    "Epoch 114, Batch 200, Loss 94.176903\n",
    "Epoch 114, Batch 300, Loss 97.953140\n",
    "Epoch 114, Batch 400, Loss 97.725220\n",
    "====> Epoch 114 Average loss: 96.5473\n",
    "Epoch 115, Batch 0, Loss 96.662239\n",
    "Epoch 115, Batch 100, Loss 97.498459\n",
    "Epoch 115, Batch 200, Loss 92.837708\n",
    "Epoch 115, Batch 300, Loss 99.160484\n",
    "Epoch 115, Batch 400, Loss 94.610481\n",
    "====> Epoch 115 Average loss: 96.5564\n",
    "Epoch 116, Batch 0, Loss 97.735893\n",
    "Epoch 116, Batch 100, Loss 95.723404\n",
    "Epoch 116, Batch 200, Loss 94.546684\n",
    "Epoch 116, Batch 300, Loss 96.761826\n",
    "Epoch 116, Batch 400, Loss 94.668251\n",
    "====> Epoch 116 Average loss: 96.4956\n",
    "Epoch 117, Batch 0, Loss 98.023209\n",
    "Epoch 117, Batch 100, Loss 98.140335\n",
    "Epoch 117, Batch 200, Loss 92.567543\n",
    "Epoch 117, Batch 300, Loss 95.709259\n",
    "Epoch 117, Batch 400, Loss 95.763458\n",
    "====> Epoch 117 Average loss: 96.5028\n",
    "Epoch 118, Batch 0, Loss 96.631645\n",
    "Epoch 118, Batch 100, Loss 93.680038\n",
    "Epoch 118, Batch 200, Loss 101.426903\n",
    "Epoch 118, Batch 300, Loss 97.317856\n",
    "Epoch 118, Batch 400, Loss 98.577423\n",
    "====> Epoch 118 Average loss: 96.4617\n",
    "Epoch 119, Batch 0, Loss 95.386238\n",
    "Epoch 119, Batch 100, Loss 91.894554\n",
    "Epoch 119, Batch 200, Loss 92.499916\n",
    "Epoch 119, Batch 300, Loss 97.703201\n",
    "Epoch 119, Batch 400, Loss 95.904602\n",
    "====> Epoch 119 Average loss: 96.4945\n",
    "Epoch 120, Batch 0, Loss 99.517807\n",
    "Epoch 120, Batch 100, Loss 94.484985\n",
    "Epoch 120, Batch 200, Loss 100.688858\n",
    "Epoch 120, Batch 300, Loss 98.294594\n",
    "Epoch 120, Batch 400, Loss 97.459961\n",
    "====> Epoch 120 Average loss: 96.4347\n",
    "Epoch 121, Batch 0, Loss 97.547012\n",
    "Epoch 121, Batch 100, Loss 96.712479\n",
    "Epoch 121, Batch 200, Loss 95.961617\n",
    "Epoch 121, Batch 300, Loss 97.076477\n",
    "Epoch 121, Batch 400, Loss 98.730598\n",
    "====> Epoch 121 Average loss: 96.4122\n",
    "Epoch 122, Batch 0, Loss 100.415970\n",
    "Epoch 122, Batch 100, Loss 95.492065\n",
    "Epoch 122, Batch 200, Loss 96.878815\n",
    "Epoch 122, Batch 300, Loss 97.320847\n",
    "Epoch 122, Batch 400, Loss 99.559601\n",
    "====> Epoch 122 Average loss: 96.3701\n",
    "Epoch 123, Batch 0, Loss 96.164429\n",
    "Epoch 123, Batch 100, Loss 100.128418\n",
    "Epoch 123, Batch 200, Loss 94.732315\n",
    "Epoch 123, Batch 300, Loss 99.867249\n",
    "Epoch 123, Batch 400, Loss 94.428490\n",
    "====> Epoch 123 Average loss: 96.4513\n",
    "Epoch 124, Batch 0, Loss 94.492020\n",
    "Epoch 124, Batch 100, Loss 96.049194\n",
    "Epoch 124, Batch 200, Loss 97.635368\n",
    "Epoch 124, Batch 300, Loss 96.565598\n",
    "Epoch 124, Batch 400, Loss 99.684807\n",
    "====> Epoch 124 Average loss: 96.3784\n",
    "Epoch 125, Batch 0, Loss 96.504242\n",
    "Epoch 125, Batch 100, Loss 98.424622\n",
    "Epoch 125, Batch 200, Loss 94.027039\n",
    "Epoch 125, Batch 300, Loss 92.760490\n",
    "Epoch 125, Batch 400, Loss 96.046936\n",
    "====> Epoch 125 Average loss: 96.3260\n",
    "Epoch 126, Batch 0, Loss 97.023003\n",
    "Epoch 126, Batch 100, Loss 94.388542\n",
    "Epoch 126, Batch 200, Loss 95.004135\n",
    "Epoch 126, Batch 300, Loss 92.949036\n",
    "Epoch 126, Batch 400, Loss 95.937775\n",
    "====> Epoch 126 Average loss: 96.3524\n",
    "Epoch 127, Batch 0, Loss 97.532936\n",
    "Epoch 127, Batch 100, Loss 98.256409\n",
    "Epoch 127, Batch 200, Loss 95.563408\n",
    "Epoch 127, Batch 300, Loss 95.170753\n",
    "Epoch 127, Batch 400, Loss 97.035217\n",
    "====> Epoch 127 Average loss: 96.3313\n",
    "Epoch 128, Batch 0, Loss 96.691750\n",
    "Epoch 128, Batch 100, Loss 94.071091\n",
    "Epoch 128, Batch 200, Loss 97.799782\n",
    "Epoch 128, Batch 300, Loss 89.837204\n",
    "Epoch 128, Batch 400, Loss 97.294739\n",
    "====> Epoch 128 Average loss: 96.3508\n",
    "Epoch 129, Batch 0, Loss 96.807137\n",
    "Epoch 129, Batch 100, Loss 98.178917\n",
    "Epoch 129, Batch 200, Loss 98.515137\n",
    "Epoch 129, Batch 300, Loss 94.609100\n",
    "Epoch 129, Batch 400, Loss 101.218941\n",
    "====> Epoch 129 Average loss: 96.3257\n",
    "Epoch 130, Batch 0, Loss 95.389862\n",
    "Epoch 130, Batch 100, Loss 97.127823\n",
    "Epoch 130, Batch 200, Loss 95.102814\n",
    "Epoch 130, Batch 300, Loss 102.633026\n",
    "Epoch 130, Batch 400, Loss 95.997772\n",
    "====> Epoch 130 Average loss: 96.2877\n",
    "Epoch 131, Batch 0, Loss 96.402855\n",
    "Epoch 131, Batch 100, Loss 94.710922\n",
    "Epoch 131, Batch 200, Loss 93.718224\n",
    "Epoch 131, Batch 300, Loss 91.354691\n",
    "Epoch 131, Batch 400, Loss 96.431381\n",
    "====> Epoch 131 Average loss: 96.2974\n",
    "Epoch 132, Batch 0, Loss 92.811356\n",
    "Epoch 132, Batch 100, Loss 94.710892\n",
    "Epoch 132, Batch 200, Loss 95.261124\n",
    "Epoch 132, Batch 300, Loss 96.702057\n",
    "Epoch 132, Batch 400, Loss 95.952896\n",
    "====> Epoch 132 Average loss: 96.3145\n",
    "Epoch 133, Batch 0, Loss 100.455322\n",
    "Epoch 133, Batch 100, Loss 93.953430\n",
    "Epoch 133, Batch 200, Loss 99.064499\n",
    "Epoch 133, Batch 300, Loss 92.390785\n",
    "Epoch 133, Batch 400, Loss 100.057922\n",
    "====> Epoch 133 Average loss: 96.2446\n",
    "Epoch 134, Batch 0, Loss 93.933929\n",
    "Epoch 134, Batch 100, Loss 96.988495\n",
    "Epoch 134, Batch 200, Loss 94.495926\n",
    "Epoch 134, Batch 300, Loss 101.988693\n",
    "Epoch 134, Batch 400, Loss 96.496269\n",
    "====> Epoch 134 Average loss: 96.2055\n",
    "Epoch 135, Batch 0, Loss 91.454254\n",
    "Epoch 135, Batch 100, Loss 96.158203\n",
    "Epoch 135, Batch 200, Loss 96.250587\n",
    "Epoch 135, Batch 300, Loss 96.751671\n",
    "Epoch 135, Batch 400, Loss 96.570312\n",
    "====> Epoch 135 Average loss: 96.2224\n",
    "Epoch 136, Batch 0, Loss 95.347694\n",
    "Epoch 136, Batch 100, Loss 97.937988\n",
    "Epoch 136, Batch 200, Loss 96.141556\n",
    "Epoch 136, Batch 300, Loss 97.257278\n",
    "Epoch 136, Batch 400, Loss 98.438988\n",
    "====> Epoch 136 Average loss: 96.2359\n",
    "Epoch 137, Batch 0, Loss 98.532173\n",
    "Epoch 137, Batch 100, Loss 93.835068\n",
    "Epoch 137, Batch 200, Loss 97.606827\n",
    "Epoch 137, Batch 300, Loss 95.442841\n",
    "Epoch 137, Batch 400, Loss 95.502525\n",
    "====> Epoch 137 Average loss: 96.1961\n",
    "Epoch 138, Batch 0, Loss 94.832565\n",
    "Epoch 138, Batch 100, Loss 97.847694\n",
    "Epoch 138, Batch 200, Loss 96.121544\n",
    "Epoch 138, Batch 300, Loss 95.493103\n",
    "Epoch 138, Batch 400, Loss 97.715881\n",
    "====> Epoch 138 Average loss: 96.1619\n",
    "Epoch 139, Batch 0, Loss 99.222961\n",
    "Epoch 139, Batch 100, Loss 96.204239\n",
    "Epoch 139, Batch 200, Loss 97.130409\n",
    "Epoch 139, Batch 300, Loss 95.846985\n",
    "Epoch 139, Batch 400, Loss 98.458511\n",
    "====> Epoch 139 Average loss: 96.2042\n",
    "Epoch 140, Batch 0, Loss 93.347656\n",
    "Epoch 140, Batch 100, Loss 98.590446\n",
    "Epoch 140, Batch 200, Loss 98.334770\n",
    "Epoch 140, Batch 300, Loss 93.093460\n",
    "Epoch 140, Batch 400, Loss 94.064125\n",
    "====> Epoch 140 Average loss: 96.1864\n",
    "Epoch 141, Batch 0, Loss 93.161163\n",
    "Epoch 141, Batch 100, Loss 96.771095\n",
    "Epoch 141, Batch 200, Loss 93.216080\n",
    "Epoch 141, Batch 300, Loss 99.144264\n",
    "Epoch 141, Batch 400, Loss 94.537262\n",
    "====> Epoch 141 Average loss: 96.1436\n",
    "Epoch 142, Batch 0, Loss 96.085121\n",
    "Epoch 142, Batch 100, Loss 96.307213\n",
    "Epoch 142, Batch 200, Loss 94.171875\n",
    "Epoch 142, Batch 300, Loss 97.162598\n",
    "Epoch 142, Batch 400, Loss 94.374474\n",
    "====> Epoch 142 Average loss: 96.1656\n",
    "Epoch 143, Batch 0, Loss 96.958115\n",
    "Epoch 143, Batch 100, Loss 96.023529\n",
    "Epoch 143, Batch 200, Loss 96.372726\n",
    "Epoch 143, Batch 300, Loss 97.579605\n",
    "Epoch 143, Batch 400, Loss 96.845688\n",
    "====> Epoch 143 Average loss: 96.1538\n",
    "Epoch 144, Batch 0, Loss 96.679184\n",
    "Epoch 144, Batch 100, Loss 96.710655\n",
    "Epoch 144, Batch 200, Loss 96.237617\n",
    "Epoch 144, Batch 300, Loss 98.174500\n",
    "Epoch 144, Batch 400, Loss 95.432510\n",
    "====> Epoch 144 Average loss: 96.0811\n",
    "Epoch 145, Batch 0, Loss 96.982666\n",
    "Epoch 145, Batch 100, Loss 97.095589\n",
    "Epoch 145, Batch 200, Loss 95.876488\n",
    "Epoch 145, Batch 300, Loss 95.564178\n",
    "Epoch 145, Batch 400, Loss 96.329803\n",
    "====> Epoch 145 Average loss: 96.1121\n",
    "Epoch 146, Batch 0, Loss 94.212738\n",
    "Epoch 146, Batch 100, Loss 96.899261\n",
    "Epoch 146, Batch 200, Loss 93.762688\n",
    "Epoch 146, Batch 300, Loss 99.699867\n",
    "Epoch 146, Batch 400, Loss 97.539467\n",
    "====> Epoch 146 Average loss: 96.1127\n",
    "Epoch 147, Batch 0, Loss 97.061394\n",
    "Epoch 147, Batch 100, Loss 97.888443\n",
    "Epoch 147, Batch 200, Loss 98.409515\n",
    "Epoch 147, Batch 300, Loss 97.908127\n",
    "Epoch 147, Batch 400, Loss 97.585922\n",
    "====> Epoch 147 Average loss: 96.0967\n",
    "Epoch 148, Batch 0, Loss 94.432892\n",
    "Epoch 148, Batch 100, Loss 95.512978\n",
    "Epoch 148, Batch 200, Loss 97.762665\n",
    "Epoch 148, Batch 300, Loss 95.711693\n",
    "Epoch 148, Batch 400, Loss 95.638008\n",
    "====> Epoch 148 Average loss: 96.0735\n",
    "Epoch 149, Batch 0, Loss 96.280472\n",
    "Epoch 149, Batch 100, Loss 92.676445\n",
    "Epoch 149, Batch 200, Loss 100.758919\n",
    "Epoch 149, Batch 300, Loss 94.478508\n",
    "Epoch 149, Batch 400, Loss 96.085709\n",
    "====> Epoch 149 Average loss: 96.0433\n",
    "Epoch 150, Batch 0, Loss 95.339127\n",
    "Epoch 150, Batch 100, Loss 99.151398\n",
    "Epoch 150, Batch 200, Loss 94.718781\n",
    "Epoch 150, Batch 300, Loss 96.062080\n",
    "Epoch 150, Batch 400, Loss 95.655838\n",
    "====> Epoch 150 Average loss: 96.0657\n",
    "Epoch 151, Batch 0, Loss 94.142105\n",
    "Epoch 151, Batch 100, Loss 97.695602\n",
    "Epoch 151, Batch 200, Loss 96.209732\n",
    "Epoch 151, Batch 300, Loss 92.991013\n",
    "Epoch 151, Batch 400, Loss 91.188385\n",
    "====> Epoch 151 Average loss: 96.0642\n",
    "Epoch 152, Batch 0, Loss 96.031433\n",
    "Epoch 152, Batch 100, Loss 95.646935\n",
    "Epoch 152, Batch 200, Loss 97.808853\n",
    "Epoch 152, Batch 300, Loss 95.683762\n",
    "Epoch 152, Batch 400, Loss 96.012436\n",
    "====> Epoch 152 Average loss: 96.0175\n",
    "Epoch 153, Batch 0, Loss 95.163742\n",
    "Epoch 153, Batch 100, Loss 96.585609\n",
    "Epoch 153, Batch 200, Loss 95.671265\n",
    "Epoch 153, Batch 300, Loss 92.936356\n",
    "Epoch 153, Batch 400, Loss 97.113297\n",
    "====> Epoch 153 Average loss: 96.0214\n",
    "Epoch 154, Batch 0, Loss 96.455193\n",
    "Epoch 154, Batch 100, Loss 95.252258\n",
    "Epoch 154, Batch 200, Loss 95.601761\n",
    "Epoch 154, Batch 300, Loss 94.272362\n",
    "Epoch 154, Batch 400, Loss 96.104446\n",
    "====> Epoch 154 Average loss: 96.0237\n",
    "Epoch 155, Batch 0, Loss 93.664368\n",
    "Epoch 155, Batch 100, Loss 94.075912\n",
    "Epoch 155, Batch 200, Loss 95.935509\n",
    "Epoch 155, Batch 300, Loss 100.541245\n",
    "Epoch 155, Batch 400, Loss 93.324203\n",
    "====> Epoch 155 Average loss: 95.9447\n",
    "Epoch 156, Batch 0, Loss 96.796692\n",
    "Epoch 156, Batch 100, Loss 93.478470\n",
    "Epoch 156, Batch 200, Loss 95.566071\n",
    "Epoch 156, Batch 300, Loss 99.520615\n",
    "Epoch 156, Batch 400, Loss 93.147705\n",
    "====> Epoch 156 Average loss: 95.9705\n",
    "Epoch 157, Batch 0, Loss 95.448051\n",
    "Epoch 157, Batch 100, Loss 95.905983\n",
    "Epoch 157, Batch 200, Loss 94.950546\n",
    "Epoch 157, Batch 300, Loss 100.610229\n",
    "Epoch 157, Batch 400, Loss 95.638062\n",
    "====> Epoch 157 Average loss: 95.9813\n",
    "Epoch 158, Batch 0, Loss 97.981194\n",
    "Epoch 158, Batch 100, Loss 94.639343\n",
    "Epoch 158, Batch 200, Loss 97.042770\n",
    "Epoch 158, Batch 300, Loss 96.794655\n",
    "Epoch 158, Batch 400, Loss 95.058121\n",
    "====> Epoch 158 Average loss: 95.9522\n",
    "Epoch 159, Batch 0, Loss 94.229584\n",
    "Epoch 159, Batch 100, Loss 95.616028\n",
    "Epoch 159, Batch 200, Loss 95.143135\n",
    "Epoch 159, Batch 300, Loss 90.463646\n",
    "Epoch 159, Batch 400, Loss 91.511154\n",
    "====> Epoch 159 Average loss: 95.9443\n",
    "Epoch 160, Batch 0, Loss 98.293236\n",
    "Epoch 160, Batch 100, Loss 92.677414\n",
    "Epoch 160, Batch 200, Loss 96.539101\n",
    "Epoch 160, Batch 300, Loss 94.716034\n",
    "Epoch 160, Batch 400, Loss 94.849335\n",
    "====> Epoch 160 Average loss: 95.9450\n",
    "Epoch 161, Batch 0, Loss 94.895897\n",
    "Epoch 161, Batch 100, Loss 99.304382\n",
    "Epoch 161, Batch 200, Loss 95.001465\n",
    "Epoch 161, Batch 300, Loss 96.598831\n",
    "Epoch 161, Batch 400, Loss 93.688477\n",
    "====> Epoch 161 Average loss: 95.9048\n",
    "Epoch 162, Batch 0, Loss 97.440277\n",
    "Epoch 162, Batch 100, Loss 100.664642\n",
    "Epoch 162, Batch 200, Loss 94.625153\n",
    "Epoch 162, Batch 300, Loss 98.556114\n",
    "Epoch 162, Batch 400, Loss 97.327797\n",
    "====> Epoch 162 Average loss: 95.8861\n",
    "Epoch 163, Batch 0, Loss 97.963699\n",
    "Epoch 163, Batch 100, Loss 98.500069\n",
    "Epoch 163, Batch 200, Loss 95.732086\n",
    "Epoch 163, Batch 300, Loss 98.580154\n",
    "Epoch 163, Batch 400, Loss 92.814056\n",
    "====> Epoch 163 Average loss: 95.9215\n",
    "Epoch 164, Batch 0, Loss 94.226532\n",
    "Epoch 164, Batch 100, Loss 98.153038\n",
    "Epoch 164, Batch 200, Loss 96.885872\n",
    "Epoch 164, Batch 300, Loss 93.793198\n",
    "Epoch 164, Batch 400, Loss 93.779587\n",
    "====> Epoch 164 Average loss: 95.9253\n",
    "Epoch 165, Batch 0, Loss 97.352921\n",
    "Epoch 165, Batch 100, Loss 94.425995\n",
    "Epoch 165, Batch 200, Loss 97.757233\n",
    "Epoch 165, Batch 300, Loss 94.175919\n",
    "Epoch 165, Batch 400, Loss 98.402710\n",
    "====> Epoch 165 Average loss: 95.8708\n",
    "Epoch 166, Batch 0, Loss 99.384872\n",
    "Epoch 166, Batch 100, Loss 90.947266\n",
    "Epoch 166, Batch 200, Loss 94.171928\n",
    "Epoch 166, Batch 300, Loss 100.587837\n",
    "Epoch 166, Batch 400, Loss 99.676804\n",
    "====> Epoch 166 Average loss: 95.8830\n",
    "Epoch 167, Batch 0, Loss 94.505981\n",
    "Epoch 167, Batch 100, Loss 98.171265\n",
    "Epoch 167, Batch 200, Loss 98.937225\n",
    "Epoch 167, Batch 300, Loss 94.936462\n",
    "Epoch 167, Batch 400, Loss 92.792870\n",
    "====> Epoch 167 Average loss: 95.8751\n",
    "Epoch 168, Batch 0, Loss 95.720634\n",
    "Epoch 168, Batch 100, Loss 95.885712\n",
    "Epoch 168, Batch 200, Loss 96.432404\n",
    "Epoch 168, Batch 300, Loss 93.332275\n",
    "Epoch 168, Batch 400, Loss 98.336784\n",
    "====> Epoch 168 Average loss: 95.9135\n",
    "Epoch 169, Batch 0, Loss 94.330963\n",
    "Epoch 169, Batch 100, Loss 94.813637\n",
    "Epoch 169, Batch 200, Loss 98.322441\n",
    "Epoch 169, Batch 300, Loss 98.103882\n",
    "Epoch 169, Batch 400, Loss 98.876465\n",
    "====> Epoch 169 Average loss: 95.8500\n",
    "Epoch 170, Batch 0, Loss 95.345367\n",
    "Epoch 170, Batch 100, Loss 93.327744\n",
    "Epoch 170, Batch 200, Loss 98.490921\n",
    "Epoch 170, Batch 300, Loss 92.627686\n",
    "Epoch 170, Batch 400, Loss 95.301987\n",
    "====> Epoch 170 Average loss: 95.8792\n",
    "Epoch 171, Batch 0, Loss 96.374527\n",
    "Epoch 171, Batch 100, Loss 94.132004\n",
    "Epoch 171, Batch 200, Loss 95.490227\n",
    "Epoch 171, Batch 300, Loss 97.002831\n",
    "Epoch 171, Batch 400, Loss 98.354523\n",
    "====> Epoch 171 Average loss: 95.8440\n",
    "Epoch 172, Batch 0, Loss 93.431465\n",
    "Epoch 172, Batch 100, Loss 95.930115\n",
    "Epoch 172, Batch 200, Loss 94.709595\n",
    "Epoch 172, Batch 300, Loss 95.283890\n",
    "Epoch 172, Batch 400, Loss 92.482407\n",
    "====> Epoch 172 Average loss: 95.8143\n",
    "Epoch 173, Batch 0, Loss 96.992493\n",
    "Epoch 173, Batch 100, Loss 95.425079\n",
    "Epoch 173, Batch 200, Loss 96.158783\n",
    "Epoch 173, Batch 300, Loss 93.730377\n",
    "Epoch 173, Batch 400, Loss 97.353920\n",
    "====> Epoch 173 Average loss: 95.8347\n",
    "Epoch 174, Batch 0, Loss 99.007629\n",
    "Epoch 174, Batch 100, Loss 98.824646\n",
    "Epoch 174, Batch 200, Loss 92.443321\n",
    "Epoch 174, Batch 300, Loss 94.355988\n",
    "Epoch 174, Batch 400, Loss 94.284500\n",
    "====> Epoch 174 Average loss: 95.8479\n",
    "Epoch 175, Batch 0, Loss 93.888840\n",
    "Epoch 175, Batch 100, Loss 98.705780\n",
    "Epoch 175, Batch 200, Loss 95.594604\n",
    "Epoch 175, Batch 300, Loss 96.593239\n",
    "Epoch 175, Batch 400, Loss 98.398834\n",
    "====> Epoch 175 Average loss: 95.7902\n",
    "Epoch 176, Batch 0, Loss 97.554810\n",
    "Epoch 176, Batch 100, Loss 96.570938\n",
    "Epoch 176, Batch 200, Loss 99.638885\n",
    "Epoch 176, Batch 300, Loss 95.503311\n",
    "Epoch 176, Batch 400, Loss 97.585136\n",
    "====> Epoch 176 Average loss: 95.7456\n",
    "Epoch 177, Batch 0, Loss 95.251259\n",
    "Epoch 177, Batch 100, Loss 93.684418\n",
    "Epoch 177, Batch 200, Loss 98.693039\n",
    "Epoch 177, Batch 300, Loss 97.741325\n",
    "Epoch 177, Batch 400, Loss 96.179726\n",
    "====> Epoch 177 Average loss: 95.7851\n",
    "Epoch 178, Batch 0, Loss 94.598427\n",
    "Epoch 178, Batch 100, Loss 94.240829\n",
    "Epoch 178, Batch 200, Loss 95.526741\n",
    "Epoch 178, Batch 300, Loss 98.145676\n",
    "Epoch 178, Batch 400, Loss 93.584885\n",
    "====> Epoch 178 Average loss: 95.7751\n",
    "Epoch 179, Batch 0, Loss 91.255417\n",
    "Epoch 179, Batch 100, Loss 96.064133\n",
    "Epoch 179, Batch 200, Loss 95.271881\n",
    "Epoch 179, Batch 300, Loss 96.640610\n",
    "Epoch 179, Batch 400, Loss 93.859512\n",
    "====> Epoch 179 Average loss: 95.8198\n",
    "Epoch 180, Batch 0, Loss 94.913498\n",
    "Epoch 180, Batch 100, Loss 97.233948\n",
    "Epoch 180, Batch 200, Loss 95.120560\n",
    "Epoch 180, Batch 300, Loss 96.291458\n",
    "Epoch 180, Batch 400, Loss 96.658813\n",
    "====> Epoch 180 Average loss: 95.7389\n",
    "Epoch 181, Batch 0, Loss 98.463043\n",
    "Epoch 181, Batch 100, Loss 93.596764\n",
    "Epoch 181, Batch 200, Loss 95.039871\n",
    "Epoch 181, Batch 300, Loss 93.147423\n",
    "Epoch 181, Batch 400, Loss 98.224136\n",
    "====> Epoch 181 Average loss: 95.7643\n",
    "Epoch 182, Batch 0, Loss 97.391930\n",
    "Epoch 182, Batch 100, Loss 93.194305\n",
    "Epoch 182, Batch 200, Loss 96.645966\n",
    "Epoch 182, Batch 300, Loss 95.199318\n",
    "Epoch 182, Batch 400, Loss 95.724823\n",
    "====> Epoch 182 Average loss: 95.7135\n",
    "Epoch 183, Batch 0, Loss 93.320213\n",
    "Epoch 183, Batch 100, Loss 96.168228\n",
    "Epoch 183, Batch 200, Loss 97.599350\n",
    "Epoch 183, Batch 300, Loss 97.774429\n",
    "Epoch 183, Batch 400, Loss 95.548782\n",
    "====> Epoch 183 Average loss: 95.7297\n",
    "Epoch 184, Batch 0, Loss 95.800964\n",
    "Epoch 184, Batch 100, Loss 94.973373\n",
    "Epoch 184, Batch 200, Loss 91.896393\n",
    "Epoch 184, Batch 300, Loss 95.311638\n",
    "Epoch 184, Batch 400, Loss 92.999840\n",
    "====> Epoch 184 Average loss: 95.7079\n",
    "Epoch 185, Batch 0, Loss 99.513931\n",
    "Epoch 185, Batch 100, Loss 91.466560\n",
    "Epoch 185, Batch 200, Loss 91.954849\n",
    "Epoch 185, Batch 300, Loss 93.524361\n",
    "Epoch 185, Batch 400, Loss 94.965538\n",
    "====> Epoch 185 Average loss: 95.7234\n",
    "Epoch 186, Batch 0, Loss 93.003418\n",
    "Epoch 186, Batch 100, Loss 97.370621\n",
    "Epoch 186, Batch 200, Loss 94.288696\n",
    "Epoch 186, Batch 300, Loss 99.166260\n",
    "Epoch 186, Batch 400, Loss 92.296921\n",
    "====> Epoch 186 Average loss: 95.7255\n",
    "Epoch 187, Batch 0, Loss 95.194763\n",
    "Epoch 187, Batch 100, Loss 96.943802\n",
    "Epoch 187, Batch 200, Loss 96.374069\n",
    "Epoch 187, Batch 300, Loss 94.790573\n",
    "Epoch 187, Batch 400, Loss 94.551102\n",
    "====> Epoch 187 Average loss: 95.7432\n",
    "Epoch 188, Batch 0, Loss 93.761879\n",
    "Epoch 188, Batch 100, Loss 96.778160\n",
    "Epoch 188, Batch 200, Loss 99.116852\n",
    "Epoch 188, Batch 300, Loss 95.758377\n",
    "Epoch 188, Batch 400, Loss 95.027283\n",
    "====> Epoch 188 Average loss: 95.7183\n",
    "Epoch 189, Batch 0, Loss 90.619347\n",
    "Epoch 189, Batch 100, Loss 99.103485\n",
    "Epoch 189, Batch 200, Loss 94.586533\n",
    "Epoch 189, Batch 300, Loss 98.200089\n",
    "Epoch 189, Batch 400, Loss 98.931618\n",
    "====> Epoch 189 Average loss: 95.6875\n",
    "Epoch 190, Batch 0, Loss 94.550285\n",
    "Epoch 190, Batch 100, Loss 96.290268\n",
    "Epoch 190, Batch 200, Loss 95.258423\n",
    "Epoch 190, Batch 300, Loss 94.441971\n",
    "Epoch 190, Batch 400, Loss 89.279205\n",
    "====> Epoch 190 Average loss: 95.6250\n",
    "Epoch 191, Batch 0, Loss 96.550995\n",
    "Epoch 191, Batch 100, Loss 96.204033\n",
    "Epoch 191, Batch 200, Loss 95.370361\n",
    "Epoch 191, Batch 300, Loss 94.765274\n",
    "Epoch 191, Batch 400, Loss 94.114502\n",
    "====> Epoch 191 Average loss: 95.6887\n",
    "Epoch 192, Batch 0, Loss 95.097694\n",
    "Epoch 192, Batch 100, Loss 94.793015\n",
    "Epoch 192, Batch 200, Loss 97.433578\n",
    "Epoch 192, Batch 300, Loss 94.138390\n",
    "Epoch 192, Batch 400, Loss 97.460754\n",
    "====> Epoch 192 Average loss: 95.6739\n",
    "Epoch 193, Batch 0, Loss 99.766769\n",
    "Epoch 193, Batch 100, Loss 97.121658\n",
    "Epoch 193, Batch 200, Loss 95.686157\n",
    "Epoch 193, Batch 300, Loss 95.990311\n",
    "Epoch 193, Batch 400, Loss 93.620537\n",
    "====> Epoch 193 Average loss: 95.6390\n",
    "Epoch 194, Batch 0, Loss 92.219742\n",
    "Epoch 194, Batch 100, Loss 95.224838\n",
    "Epoch 194, Batch 200, Loss 97.520966\n",
    "Epoch 194, Batch 300, Loss 93.114235\n",
    "Epoch 194, Batch 400, Loss 95.806503\n",
    "====> Epoch 194 Average loss: 95.6678\n",
    "Epoch 195, Batch 0, Loss 98.895622\n",
    "Epoch 195, Batch 100, Loss 97.124817\n",
    "Epoch 195, Batch 200, Loss 96.951950\n",
    "Epoch 195, Batch 300, Loss 97.832680\n",
    "Epoch 195, Batch 400, Loss 98.067604\n",
    "====> Epoch 195 Average loss: 95.6108\n",
    "Epoch 196, Batch 0, Loss 94.841522\n",
    "Epoch 196, Batch 100, Loss 96.897621\n",
    "Epoch 196, Batch 200, Loss 98.198364\n",
    "Epoch 196, Batch 300, Loss 95.537872\n",
    "Epoch 196, Batch 400, Loss 95.772537\n",
    "====> Epoch 196 Average loss: 95.6274\n",
    "Epoch 197, Batch 0, Loss 96.310890\n",
    "Epoch 197, Batch 100, Loss 94.132805\n",
    "Epoch 197, Batch 200, Loss 94.295334\n",
    "Epoch 197, Batch 300, Loss 93.015015\n",
    "Epoch 197, Batch 400, Loss 94.556412\n",
    "====> Epoch 197 Average loss: 95.6722\n",
    "Epoch 198, Batch 0, Loss 93.734848\n",
    "Epoch 198, Batch 100, Loss 95.779221\n",
    "Epoch 198, Batch 200, Loss 94.581894\n",
    "Epoch 198, Batch 300, Loss 93.311691\n",
    "Epoch 198, Batch 400, Loss 93.593391\n",
    "====> Epoch 198 Average loss: 95.5975\n",
    "Epoch 199, Batch 0, Loss 95.491066\n",
    "Epoch 199, Batch 100, Loss 90.493645\n",
    "Epoch 199, Batch 200, Loss 95.610428\n",
    "Epoch 199, Batch 300, Loss 94.661217\n",
    "Epoch 199, Batch 400, Loss 93.657410\n",
    "====> Epoch 199 Average loss: 95.6210\n",
    "Epoch 200, Batch 0, Loss 97.098358\n",
    "Epoch 200, Batch 100, Loss 95.999954\n",
    "Epoch 200, Batch 200, Loss 99.197968\n",
    "Epoch 200, Batch 300, Loss 94.447220\n",
    "Epoch 200, Batch 400, Loss 98.277771\n",
    "====> Epoch 200 Average loss: 95.5777\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line for line in log.split(\"\\n\") if line.startswith(\"Epoch\")]\n",
    "losses = [float(line.split(\"Loss\")[-1].lstrip()) for line in lines]\n",
    "\n",
    "half_width = 10\n",
    "smoothed_losses = [sum(losses[index:index+half_width*2]) / (2*half_width) for index in range(len(losses)-half_width)]\n",
    "\n",
    "plt.plot(losses[10:])\n",
    "plt.plot(smoothed_losses[10:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
