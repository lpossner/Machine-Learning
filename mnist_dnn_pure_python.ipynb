{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def D_ReLU(x):\n",
    "    return np.diag((x >= 0).astype(float))\n",
    "\n",
    "\n",
    "def SoftMax(x):\n",
    "    tmp = np.exp(x - np.max(x))\n",
    "    return tmp / np.sum(tmp)\n",
    "\n",
    "\n",
    "def D_SoftMax(x):\n",
    "    tmp = SoftMax(x)\n",
    "    return -1 * np.outer(tmp, tmp) + np.diag(tmp)\n",
    "\n",
    "\n",
    "def CategoricalCrossEntropy(y_pred, y_true):\n",
    "    tmp = np.clip(y_pred, 1e-8, 1-1e-8)\n",
    "    return np.mean(-1 * np.sum(y_true * np.log2(tmp) + (1 - y_true) * np.log2(1 - tmp), axis=1), axis=0)\n",
    "\n",
    "\n",
    "def D_CategoricalCrossEntropy(y_pred, y_true):\n",
    "    return y_pred - y_true\n",
    " \n",
    "\n",
    "def L2_loss(y_pred, y_true):\n",
    "    return np.mean(np.sum((y_pred - y_true)**2, axis=1), axis=0)\n",
    "\n",
    "\n",
    "def D_L2_loss(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / y_pred.shape[0]\n",
    "\n",
    "\n",
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, activation, D_activation):\n",
    "        self.weights = np.random.randn(n_outputs, n_inputs) / np.sqrt(n_inputs * n_outputs)\n",
    "        self.bias = np.random.randn(n_outputs) / np.sqrt(n_outputs)\n",
    "        self.activation = activation\n",
    "        self.D_activation = D_activation\n",
    "        self.x = None\n",
    "        self.D_y = None\n",
    "        self.weights_error_lst = []\n",
    "        self.bias_error_lst = []\n",
    "        self.weights_error_rp = 0\n",
    "        self.bias_error_rp = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        self.D_y = self.D_activation(z)\n",
    "        y = self.activation(z)\n",
    "        return y\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        bias_error = np.dot(self.D_y.T, output_error)\n",
    "        self.bias_error_lst.append(bias_error)\n",
    "        weights_error = np.outer(bias_error, self.x)\n",
    "        self.weights_error_lst.append(weights_error)\n",
    "        input_error = np.dot(self.weights.T, bias_error)\n",
    "        return input_error\n",
    "    \n",
    "    def update(self, learning_rate, adaption_rate):\n",
    "        weights_error = np.mean(np.array(self.weights_error_lst), axis=0)\n",
    "        self.weights_error_rp = np.clip(adaption_rate * self.weights_error_rp + (1 - adaption_rate) * weights_error**2, 1e-9, np.inf)\n",
    "        self.weights -= learning_rate / (np.sqrt(self.weights_error_rp)) * weights_error\n",
    "        self.weights_error_lst = []\n",
    "        bias_error = np.mean(np.array(self.bias_error_lst), axis=0)\n",
    "        self.bias_error_rp = np.clip(adaption_rate * self.bias_error_rp + (1 - adaption_rate) * bias_error**2, 1e-9, np.inf)\n",
    "        self.bias -= learning_rate / (np.sqrt(self.bias_error_rp)) * bias_error\n",
    "        self.bias_error_lst = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = None\n",
    "        self.D_y = None\n",
    "        self.weights_error_lst = []\n",
    "        self.bias_error_lst = []\n",
    "        self.weights_error_rp = 0\n",
    "        self.bias_error_rp = 0\n",
    "\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layer_lst = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred_lst = []\n",
    "        for x in X:\n",
    "            y_pred_lst.append(self._forward(x))\n",
    "        y_pred = np.array(y_pred_lst)\n",
    "        return y_pred\n",
    "\n",
    "    def _forward(self, x):\n",
    "        y_pred = x\n",
    "        for layer in self.layer_lst:\n",
    "            y_pred = layer.forward(y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def _backward(self, y):\n",
    "        y_pred = y\n",
    "        for layer in reversed(self.layer_lst):\n",
    "            y_pred = layer.backward(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def _update(self, learning_rate, adaption_rate):\n",
    "        for layer in self.layer_lst:\n",
    "            layer.update(learning_rate, adaption_rate)\n",
    "\n",
    "    def _reset(self):\n",
    "        for layer in self.layer_lst:\n",
    "            layer.reset()\n",
    "\n",
    "    def fit(self, X_train, y_train, learning_rate=1e-1, adaption_rate=0.9, N_epochs=1000, N_batch=100):\n",
    "        self._reset()\n",
    "        for index_epoch in range(N_epochs):\n",
    "            batch_indices = np.random.randint(0, X_train.shape[0] - 1, [N_batch])\n",
    "            X_batch = X_train[batch_indices]\n",
    "            y_batch = y_train[batch_indices]\n",
    "            y_pred = np.full_like(y_batch, fill_value=np.nan)\n",
    "            for index_batch, (x, y) in enumerate(zip(X_batch, y_batch)):\n",
    "                y_pred[index_batch, :] = self._forward(x)\n",
    "                self._backward(D_CategoricalCrossEntropy(y_pred[index_batch, :], y))\n",
    "            self._update(learning_rate, adaption_rate)\n",
    "            loss = CategoricalCrossEntropy(y_pred, y_batch)\n",
    "            print(\"Epoch {}/{} Loss: {}\".format(int(index_epoch + 1), N_epochs, loss))\n",
    "\n",
    "\n",
    "N_epochs = 500\n",
    "N_batch = 100\n",
    "N_inputs = train_images.shape[1]\n",
    "N_outputs = train_labels.shape[1]\n",
    "N_hidden_layer = 2\n",
    "learning_rate = 1e-3\n",
    "adaption_rate = 0.9\n",
    "\n",
    "network = Network()\n",
    "network.layer_lst.append(DenseLayer(N_inputs, N_inputs, ReLU, D_ReLU))\n",
    "network.layer_lst.extend([DenseLayer(N_inputs, N_inputs, ReLU, D_ReLU) for _ in range(N_hidden_layer)])\n",
    "network.layer_lst.append(DenseLayer(N_inputs, N_outputs, SoftMax, D_SoftMax))\n",
    "network.fit(train_images, train_labels, learning_rate, adaption_rate, N_epochs, N_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = network.predict(test_images)\n",
    "accuracy = np.sum(np.argmax(predicted_labels, axis=1) == np.argmax(test_labels, axis=1)) / predicted_labels.shape[0] * 100\n",
    "print(\"Accuracy: {:.1f}%\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
